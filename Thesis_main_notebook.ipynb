{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multivariate Time Series Prediction Using Transformers Architecture\n",
    "\n",
    "The following Notebook shows the coding part of my Bachelor Thesis for the Information and Communication Systems and Services bachelor degree in the University of Applied Science Technikum Wien.\n",
    "\n",
    "Author: Sergio Tallo Torres\n",
    "Date: May 2022"
   ],
   "metadata": {
    "id": "EFhSljeOCnRw",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0esqvQHT2922",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# First: load imports needed for the project and project preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from platform import python_version\n",
    "\n",
    "import utils_bsc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Device: GPU =', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Device: CPU')\n",
    "\n",
    "saved_results = 'training_results'\n",
    "\n",
    "# Check whether the directory where the results files must be saved exists or not\n",
    "if not os.path.exists(saved_results):\n",
    "  # Create a new directory because it does not exist\n",
    "  os.makedirs(saved_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versions of packages:\n",
      "Python: 3.9.2\n",
      "Pandas: 1.3.3\n",
      "Numpy: 1.20.3\n",
      "PyTorch: 1.10.0\n",
      "Sklearn: 1.0\n",
      "seaborn: 0.11.2\n",
      "scipy: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "print('versions of packages:')\n",
    "print(f'Python: {python_version()}')\n",
    "print(f'Pandas: {pd.__version__}')\n",
    "print(f'Numpy: {np.__version__}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Sklearn: {sklearn.__version__}')\n",
    "print(f'seaborn: {sns.__version__}')\n",
    "print(f'scipy: {scipy.__version__}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This cell is necessary to use this notebook in google colab\n",
    "# If you are running this notebook in colab, please change colab to True\n",
    "\n",
    "colab = False\n",
    "\n",
    "if colab is True:\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    if cwd != \"/content/Bsc_Thesis\":\n",
    "        ! git clone https://github.com/SergioTallo/Bsc_Thesis.git\n",
    "        % cd Bsc_Thesis\n",
    "\n",
    "    print(cwd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data loading and preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we should create a dataset with all the data stored in the .csv file\n",
    "\n",
    "Description of the data:\n",
    "\n",
    "*   time: Timestamp (YYYY-MM-DD HH:MM:SS)\n",
    "*   PLN1: Power in the phase 1 (W)\n",
    "*   PLN2: Power in the phase 2 (W)\n",
    "*   PLN3: Power in the phase 3 (W)\n",
    "*   ULL1: Current Voltage between 2 phases (V)\n",
    "*   ULL2: Current Voltage between 2 phases (V)\n",
    "*   ULL3: Current Voltage between 2 phases (V)\n",
    "*   COS_PHI1: Phase shift (Cos)\n",
    "*   COS_PHI2: Phase shift (Cos)\n",
    "*   COS_PHI3: Phase shift (Cos)\n",
    "*   FREQ: Electricity Frequency (Hz)\n",
    "*   RC_DC: Fault currents\n",
    "*   RC_AC: Fault currents\n",
    "*   RC_50Hz: Fault currents\n",
    "*   RC_150Hz: Fault currents\n",
    "*   RC_<100Hz: Fault currents\n",
    "*   RC_100Hz-1kHz: Fault currents\n",
    "*   RC_>10kHz: Fault currents\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data_factory.csv')\n",
    "dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have the dataset, we should prepare it. Finding the missing or the NaN values and replace them with suitable values (in this case we use the value of the previous elemnt in the sequence)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace all mising values with NaN\n",
    "dataset = dataset.replace(' ', np.nan)\n",
    "# Search for all the rows with NaN values\n",
    "nan_values = dataset[dataset.isna().any(axis=1)]\n",
    "# Print the shape to know how many are there\n",
    "print(f'Number of rows with NaN values before cleaning: {nan_values.shape[0]}') \n",
    "\n",
    "# Fill all NaN values with the previous row value\n",
    "dataset_clean = dataset.fillna(method='ffill')\n",
    "\n",
    "# Check that there isn't any NaN values\n",
    "nan_values = dataset_clean[dataset_clean.isna().any(axis=1)]\n",
    "# Print the shape to know how many are there\n",
    "print(f'Number of rows with NaN values after cleaning: {nan_values.shape[0]}') \n",
    "\n",
    "#Total number of samples\n",
    "print(f'Total number of samples: {dataset_clean.shape[0]}')\n",
    "print(f'Number of features: {dataset_clean.shape[1]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Distribution of the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we look at the distribution of the different features of the data over different time intervals.\n",
    "First we take a look of the min and max values, mean and median value and the standard deviation of every feature."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_data = False\n",
    "print_graphs = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if print_data is True:\n",
    "  for column in dataset_clean.columns:\n",
    "    if column == 'time':\n",
    "      print(column)\n",
    "      print('Min value: ', dataset_clean[column].min())\n",
    "      print('Max value: ', dataset_clean[column].max())\n",
    "      print('')\n",
    "    else:\n",
    "      print(column)\n",
    "      print('Min value: ', dataset_clean[column].min())\n",
    "      print('Max value: ', dataset_clean[column].max())\n",
    "      print('Mean value: ', dataset_clean[column].mean())\n",
    "      print('Median value: ', dataset_clean[column].median())\n",
    "      print('Standard deviation: ', dataset_clean[column].std())\n",
    "      print('')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if print_graphs is True:\n",
    "\n",
    "  for i, column in enumerate(dataset_clean.columns):\n",
    "    if i > 0:\n",
    "      # Feature in a weekly interval\n",
    "      utils_bsc.week_plot(dataset_clean, i, column)\n",
    "      # Feature in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "      utils_bsc.daily_plot(dataset_clean, i, column)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We print some graphs showing the density distribution of every feature\n",
    "if print_graphs is True:\n",
    "  for column in tqdm(dataset_clean.columns):\n",
    "    if column != 'time':\n",
    "      sns.displot(dataset_clean, x=column, kind=\"kde\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After looking to the different data graphs i notice there two very different \"time slots\" when the data differs. One is Weekdays between 4:00 and 19:30. The other is Weekdays bewteen 19:30 and 4:00 and Weekends."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We create two extra data sets, one with the weekdays between 4:00 and 18:30 and one with the rest.\n",
    "dataset_clean_time = pd.to_datetime(dataset_clean['time'])\n",
    "\n",
    "day_mask = dataset_clean_time.dt.day_name()\n",
    "\n",
    "time_mask = (dataset_clean_time.dt.hour >= 4) & ((dataset_clean_time.dt.hour < 19) | ((dataset_clean_time.dt.hour == 19) & (dataset_clean_time.dt.minute <= 30))) & ((day_mask == ('Monday')) | (day_mask == ('Tuesday')) | (day_mask == ('Wednesday')) | (day_mask == ('Thursday')) | (day_mask == ('Friday')))\n",
    "\n",
    "dataset_weekdays = dataset_clean[time_mask]\n",
    "\n",
    "for i in range(len(time_mask)):\n",
    "  if time_mask[i] == False:\n",
    "    time_mask[i] = True\n",
    "  elif time_mask[i] == True:\n",
    "    time_mask[i] = False\n",
    "\n",
    "dataset_weekend = dataset_clean[time_mask]\n",
    "\n",
    "print(f'Weekdays dataset size: {len(dataset_weekdays)}')\n",
    "print(f'Weekend dataset size: {len(dataset_weekend)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if print_graphs is True:\n",
    "  for column in tqdm(dataset_weekdays.columns):\n",
    "    if column != 'time':\n",
    "      sns.displot(dataset_weekdays, x=column, kind=\"kde\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if print_graphs is True:\n",
    "  for column in tqdm(dataset_weekend.columns):\n",
    "    if column != 'time':\n",
    "      sns.displot(dataset_weekend, x=column, kind=\"kde\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this time we have three different datasets:\n",
    "\n",
    "* dataset_clean (Whole dataset)\n",
    "* dataset_weekdays (Entries from weekdays from 4:00 to 19:30)\n",
    "* dataset_weekend (Entries from Weekends and from weekdays from 19:30 to 4:00)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset normalisation\n",
    "\n",
    "The scale of the data of the different features is very different. Its better to have all of the features in the same scale. Therefore we perform a data normalisation. We choose to do a mean/stddev normalisation. We substract from every value the mean value of the feature and divide the result value by the std dev of this specific feature to have feature values with mean 0 and stddev of 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform the data normalisation in the whole dataset. We can print the distribution of the data if we want.\n",
    "dataset_norm = utils_bsc.normalize_mean_std_dataset(dataset_clean)\n",
    "\n",
    "print_graphs = False\n",
    "\n",
    "if print_graphs is True:\n",
    "  for column in tqdm(dataset_norm.columns):\n",
    "    if column != 'time':\n",
    "      sns.displot(dataset_norm, x=column, kind=\"kde\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform the data normalisation in the weekdays dataset. We can print the distribution of the data if we want.\n",
    "dataset_weekdays_norm = utils_bsc.normalize_mean_std_dataset(dataset_weekdays)\n",
    "\n",
    "print_graphs = False\n",
    "\n",
    "if print_graphs is True:\n",
    "  for column in tqdm(dataset_weekdays_norm.columns):\n",
    "    if column != 'time':\n",
    "      sns.displot(dataset_weekdays_norm, x=column, kind=\"kde\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform the data normalisation in the weekdays dataset. We can print the distribution of the data if we want.\n",
    "dataset_weekend_norm = utils_bsc.normalize_mean_std_dataset(dataset_weekend)\n",
    "\n",
    "print_graphs = False\n",
    "\n",
    "if print_graphs is True:\n",
    "  for column in tqdm(dataset_weekend_norm.columns):\n",
    "    if column != 'time':\n",
    "      sns.displot(dataset_weekend_norm, x=column, kind=\"kde\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_norm.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_weekdays_norm.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_weekend_norm.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this moment we have six different datasets to use:\n",
    "* dataset_clean (Whole dataset)\n",
    "* dataset_weekdays (Entries from weekdays from 4:00 to 19:30)\n",
    "* dataset_weekend (Entries from Weekends and from weekdays from 19:30 to 4:00)\n",
    "* dataset_norm (Whole dataset, mean/stddev normalised)\n",
    "* dataset_weekdays_norm (Entries from weekdays from 4:00 to 19:30, mean/stddev normalised)\n",
    "* dataset_weekend_norm (Entries from Weekends and from weekdays from 19:30 to 4:00, mean/stddev normalised)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We calculate the pearson correlation of all features and plot them into a heat map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlations = []\n",
    "matrix = []\n",
    "\n",
    "for i in dataset_norm.columns[1:]:\n",
    "  feature = []\n",
    "  for j in dataset_norm.columns[1:]:\n",
    "    print(f'Correlation between {i} and {j}')\n",
    "    correlation = stats.pearsonr(dataset_norm[i], dataset_norm[j])[0]\n",
    "    if i != j:\n",
    "      correlations.append(abs(correlation))\n",
    "      feature.append(abs(correlation))\n",
    "      print(correlation)\n",
    "  print(f'Mean of {i} correlations: {np.mean(feature)}')\n",
    "  print('')\n",
    "  matrix.append(feature)\n",
    "\n",
    "print(f'Mean of all correlations: {np.mean(correlations)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Features correlations heat map\n",
    "\n",
    "corr = dataset_norm.corr()\n",
    "sns.heatmap(corr, cmap=\"Blues\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Covariance matrix, eigenvalues and explained variance\n",
    "\n",
    "covmatrix = dataset_norm.cov()\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covmatrix)\n",
    "\n",
    "acc = 0\n",
    "\n",
    "acc_variance = []\n",
    "\n",
    "for i, eigen in enumerate(eigenvalues):\n",
    "  acc += eigen/np.sum(eigenvalues)\n",
    "  acc_variance.append(acc)\n",
    "  print(f'Explained_variance {i +1} principal component: {eigen/np.sum(eigenvalues)} (accumulated {round(acc, 4)})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "a = acc_variance\n",
    "\n",
    "b = [i +1 for i in range(len(acc_variance))]\n",
    "plt.title('Explained variance over number of principal components')\n",
    "plt.xlabel('n principal components')\n",
    "plt.xticks(b)\n",
    "plt.ylabel('explained variance')\n",
    "plt.bar(b, a)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a Baseline Model\n",
    "\n",
    "I am taking the Last step as prediction of all features to create a baselinemodel. I will use this baseline model to compare the results of the actual model with it. Everything that works better than this baseline model could be an improvement."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loader_train, loader_test = utils_bsc.create_dataloaders(dataset=dataset_norm, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Baseline model based in: Output element is the Input element"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses_train = []\n",
    "\n",
    "for i in loader_train:\n",
    "  output = i[0]\n",
    "  target = i[1]\n",
    "  loss = criterion(output, target)\n",
    "  losses_train.append(loss.item())\n",
    "\n",
    "losses_test = []\n",
    "\n",
    "for i in loader_test:\n",
    "  output = i[0]\n",
    "  target = i[1]\n",
    "  loss = criterion(output, target)\n",
    "  losses_test.append(loss.item())\n",
    "\n",
    "# save to npy file to keep track of the results and to print graphs\n",
    "np.save(saved_results + '/baseline_train.npy', losses_train)\n",
    "np.save(saved_results + '/baseline_test.npy', losses_test)\n",
    "\n",
    "if colab is True:\n",
    "    files.download(saved_results + '/baseline_train.npy')\n",
    "    files.download(saved_results + '/baseline_test.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Training set\")\n",
    "print(\"Mean Loss of baselinemodel: \", np.mean(losses_train))\n",
    "print(\"Standard deviation Loss of baselinemodel: \", np.std(losses_train))\n",
    "print('\\n')\n",
    "print(\"Test set\")\n",
    "print(\"Mean Loss of baselinemodel: \", np.mean(losses_test))\n",
    "print(\"Standard deviation Loss of baselinemodel: \", np.std(losses_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a second baseline model based on a FFN. It predicts output element based on input element"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_train_FFN = False\n",
    "\n",
    "# Create model FFN instance\n",
    "model_FFN = utils_bsc.ANN_relu(18, 18).to(device)\n",
    "\n",
    "print(f'Model: {type(model_FFN).__name__}')\n",
    "print(f'{utils_bsc.count_parameters(model_FFN)} trainable parameters.')\n",
    "\n",
    "# Define Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define Optimizer\n",
    "learning_rate=0.01\n",
    "optimizer_whole = torch.optim.SGD(model_FFN.parameters(), lr=learning_rate)\n",
    "\n",
    "if start_train_FFN is True:\n",
    "    n_epochs = 10\n",
    "\n",
    "    params_not_trained_whole = model_FFN.parameters()\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    best_results , train_losses_FFN, test_losses_FFN = utils_bsc.train_FFN(model_FFN, criterion, optimizer_whole, loader_train, loader_test, n_epochs)\n",
    "\n",
    "    model_FFN = best_results[0]\n",
    "    best_train_loss = best_results[1]\n",
    "    best_test_loss = best_results[2]\n",
    "    best_epoch_number = best_results[3]\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    time_diff = (end_time - start_time)\n",
    "    execution_time = time_diff.total_seconds()\n",
    "\n",
    "    print(f'Best test loss at epoch {best_epoch_number}')\n",
    "    print(f'Train Loss: {best_train_loss}')\n",
    "    print(f'Test Loss: {best_test_loss}')\n",
    "    print(f'\\nTraining time for {n_epochs} epochs: {execution_time} seconds')\n",
    "\n",
    "\n",
    "    # save to npy file\n",
    "    np.save(saved_results + '/FFN_train.npy', train_losses_FFN)\n",
    "    np.save(saved_results + '/FFN_test.npy', test_losses_FFN)\n",
    "    torch.save(model_FFN, saved_results + '/model_FFN.pt')\n",
    "\n",
    "    if colab is True:\n",
    "\n",
    "        from google.colab import files\n",
    "\n",
    "        files.download(saved_results + '/FFN_train.npy')\n",
    "        files.download(saved_results + '/FFN_test.npy')\n",
    "        files.download(saved_results + '/model_FFN.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if start_train_FFN is True:\n",
    "    baseline_loss = [np.mean(losses_train) for i in range(len(train_losses_FFN))]\n",
    "    utils_bsc.print_results_training(train_loss=train_losses_FFN, test_loss=test_losses_FFN, test_loss_baseline=baseline_loss, baseline_label='Baseline', title=\"Full Forward Neural Network train results\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer Model settings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we define a class with the transformer model that we are going to use:\n",
    "\n",
    "Using the already written pytorch library for Transformers:\n",
    "\n",
    "1) torch.nn.TransformerEncoderLayer (https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)\n",
    "\n",
    "*   d_model –> the number of expected features in the input (required).\n",
    "*   nhead –> the number of heads in the multiheadattention models (required).\n",
    "*   dropout –> the dropout value (default=0.1).\n",
    "*   activation –> the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. (default: relu)\n",
    "*   layer_norm_eps –> the eps value in layer normalization components (default=1e-5).\n",
    "*   batch_first –> If True, then the input and output tensors are provided as (batch, seq, feature). (default: False)\n",
    "*   norm_first –> if True, layer norm is done prior to attention and feedforward operations, respectivaly. Otherwise it’s done after. (default: False (after))\n",
    "\n",
    "2) torch.nn.TransformerDecoderLayer\n",
    "\n",
    "* d_model –> the number of expected features in the input (required).\n",
    "* nhead –> the number of heads in the multiheadattention models (required).\n",
    "* dim_feedforward –> the dimension of the feedforward network model (default=2048).\n",
    "* dropout –> the dropout value (default=0.1).\n",
    "* activation –> the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu\n",
    "* layer_norm_eps –> the eps value in layer normalization components (default=1e-5).\n",
    "* batch_first –> If True, then the input and output tensors are provided as (batch, seq, feature). Default: False.\n",
    "* norm_first –> if True, layer norm is done prior to self attention, multihead attention and feedforward operations, respectivaly. Otherwise it’s done after. Default: False (after).\n",
    "\n",
    "3) torch.nn.TransformerEncoder\n",
    "\n",
    "* encoder_layer –> an instance of the TransformerEncoderLayer() class (required).\n",
    "* num_layers –> the number of sub-encoder-layers in the encoder (required).\n",
    "* norm –> the layer normalization component (optional).\n",
    "\n",
    "\n",
    "4) torch.nn.TransformerDecoder\n",
    "\n",
    "* decoder_layer – an instance of the TransformerDecoderLayer() class (required).\n",
    "* num_layers – the number of sub-decoder-layers in the decoder (required).\n",
    "* norm – the layer normalization component (optional).\n",
    "\n",
    "We should define an optimizer too.\n",
    "For this, we use the pytorch library:\n",
    "\n",
    "* SGD –> Stochastic gradient descent.\n",
    "\n",
    "1) torch.optim.SDG (https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)\n",
    "\n",
    "* params (iterable) – iterable of parameters to optimize or dicts defining parameter groups\n",
    "* lr (float) – learning rate\n",
    "* momentum (float, optional) – momentum factor (default: 0)\n",
    "* weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\n",
    "* dampening (float, optional) – dampening for momentum (default: 0)\n",
    "* nesterov (bool, optional) – enables Nesterov momentum (default: False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_results_transformers ={}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train Transformer 'Vanilla' model, with standard hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = {'vanilla': [6, 1, 6, 2048, 'SGD', 0.01, None, True, 30, 16]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_results_transformers = utils_bsc.define_train_transformers(models, device, dataset_norm, training_results_transformers, saved_results, colab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if models['vanilla'][7] is True:\n",
    "\n",
    "    baseline_loss = [np.mean(i) for i in test_losses_FFN]\n",
    "    utils_bsc.print_results_training(train_loss=training_results_transformers['vanilla'][4], test_loss=training_results_transformers['vanilla'][5], test_loss_baseline=baseline_loss, baseline_label='FFN Test Loss', title=\"Training results \" + 'vanilla' + \" Transformer (\" + str(models['vanilla'][0]) + \" encoder layers, \" + str(models['vanilla'][1]) + \" decoder layer, \" + str(models['vanilla'][2]) + \" heads. \" + models['vanilla'][4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['vanilla'][7] = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train same transformer model but this time with ADAM optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['ADAM'] = [6, 1, 6, 2048, 'ADAM', 0.001, None, True, 30, 16]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_results_transformers = utils_bsc.define_train_transformers(models, device, dataset_norm, training_results_transformers, saved_results, colab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if models['ADAM'][7] is True:\n",
    "\n",
    "    baseline_loss = [np.mean(i) for i in training_results_transformers['vanilla'][5]]\n",
    "    utils_bsc.print_results_training(train_loss=training_results_transformers['ADAM'][4], test_loss=training_results_transformers['ADAM'][5], test_loss_baseline=baseline_loss, baseline_label='Vanilla transformer Test Loss', title=\"Training results \" + 'ADAM' + \" Transformer (\" + str(models['ADAM'][0]) + \" encoder layers, \" + str(models['ADAM'][1]) + \" decoder layer, \" + str(models['ADAM'][2]) + \" heads. \" + models['ADAM'][4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['ADAM'][7] = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train same transformer but with SGD with momentum as optimiser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['Momentum'] = [6, 1, 6, 2048, 'SGD', 0.001, 0.9, True, 30, 16]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_results_transformers = utils_bsc.define_train_transformers(models, device, dataset_norm, training_results_transformers, saved_results, colab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if models['Momentum'][7] is True:\n",
    "\n",
    "    baseline_loss = [np.mean(i) for i in training_results_transformers['vanilla'][5]]\n",
    "    utils_bsc.print_results_training(train_loss=training_results_transformers['Momentum'][4], test_loss=training_results_transformers['Momentum'][5], test_loss_baseline=baseline_loss, baseline_label='Vanilla transformer Test Loss', title=\"Training results \" + 'Momentum' + \" Transformer (\" + str(models['Momentum'][0]) + \" encoder layers, \" + str(models['Momentum'][1]) + \" decoder layer, \" + str(models['Momentum'][2]) + \" heads. \" + models['Momentum'][4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['Momentum'][7] = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the smallest transformer model with SGD with momentum as optimiser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['smallest'] = [1, 1, 1, 512, 'SGD', 0.001, 0.9, True, 30, 16]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_results_transformers = utils_bsc.define_train_transformers(models, device, dataset_norm, training_results_transformers, saved_results, colab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if models['smallest'][7] is True:\n",
    "\n",
    "    baseline_loss = [np.mean(i) for i in training_results_transformers['vanilla'][5]]\n",
    "    utils_bsc.print_results_training(train_loss=training_results_transformers['smallest'][4], test_loss=training_results_transformers['smallest'][5], test_loss_baseline=baseline_loss, baseline_label='Vanilla transformer Test Loss', title=\"Training results \" + 'smallest' + \" Transformer (\" + str(models['smallest'][0]) + \" encoder layers, \" + str(models['smallest'][1]) + \" decoder layer, \" + str(models['smallest'][2]) + \" heads. \" + models['smallest'][4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['smallest'][7] = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a bigger transformer model with SGD with momentum as optimiser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['bigger'] = [10, 5, 9, 4096, 'SGD', 0.001, 0.9, True, 30, 16]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_results_transformers = utils_bsc.define_train_transformers(models, device, dataset_norm, training_results_transformers, saved_results, colab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if models['bigger'][7] is True:\n",
    "    baseline_loss = [np.mean(i) for i in training_results_transformers['vanilla'][5]]\n",
    "    utils_bsc.print_results_training(train_loss=training_results_transformers['bigger'][4], test_loss=training_results_transformers['bigger'][5], test_loss_baseline=baseline_loss, baseline_label='Vanilla transformer Test Loss', title=\"Training results \" + 'bigger' + \" Transformer (\" + str(models['bigger'][0]) + \" encoder layers, \" + str(models['bigger'][1]) + \" decoder layer, \" + str(models['bigger'][2]) + \" heads. \" + models['bigger'][4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['bigger'][7] = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try with different sequence lengths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models['seq_15'] = [6, 1, 6, 2048, 'SGD', 0.001, 0.9, True, 15, 16]\n",
    "models['seq_60'] = [6, 1, 6, 2048, 'SGD', 0.001, 0.9, True, 60, 16]\n",
    "models['seq_2'] = [6, 1, 6, 2048, 'SGD', 0.001, 0.9, True, 2, 16]\n",
    "models['seq_20'] = [6, 1, 6, 2048, 'SGD', 0.001, 0.9, True, 20, 16]\n",
    "models['seq_10'] = [6, 1, 6, 2048, 'SGD', 0.001, 0.9, True, 10, 16]\n",
    "models['seq_120'] = [6, 1, 6, 2048, 'SGD', 0.001, 0.9, True, 120, 16]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_results_transformers = utils_bsc.define_train_transformers(models, device, dataset_norm, training_results_transformers, saved_results, colab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "bsc_arbeit.ipynb",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python392jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}