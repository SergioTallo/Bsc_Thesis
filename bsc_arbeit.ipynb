{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0esqvQHT2922"
   },
   "source": [
    "# First load imports needed for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VCwEuYFk2923",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import device, tensor\n",
    "import torch.nn as nn\n",
    "import utils_bsc\n",
    "from TransformerTallo import Transformer as Transformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQPvc4I-4LY5",
    "outputId": "9c2ad6f0-2602-447c-b8c3-8446df8d214c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versions of packages:\n",
      "Python: 3.9.2\n",
      "Pandas: 1.3.3\n",
      "Numpy: 1.20.3\n",
      "PyTorch: 1.10.0\n",
      "Sklearn: 1.0\n"
     ]
    }
   ],
   "source": [
    "utils_bsc.print_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "bjUFrMX32925"
   },
   "source": [
    "Now, we should create a dataset with all the data stored in the .csv file\n",
    "\n",
    "Description of the data:\n",
    "\n",
    "*   time: Timestamp (YYYY-MM-DD HH:MM:SS)\n",
    "*   PLN1: Power in the phase 1 (W)\n",
    "*   PLN2: Power in the phase 2 (W)\n",
    "*   PLN3: Power in the phase 3 (W)\n",
    "*   ULL1: Current Voltage between 2 phases (V)\n",
    "*   ULL2: Current Voltage between 2 phases (V)\n",
    "*   ULL3: Current Voltage between 2 phases (V)\n",
    "*   COS_PHI1: Phase shift (Cos)\n",
    "*   COS_PHI2: Phase shift (Cos)\n",
    "*   COS_PHI3: Phase shift (Cos)\n",
    "*   FREQ: Electricity Frequency (Hz)\n",
    "*   RC_DC: Fault currents\n",
    "*   RC_AC: Fault currents\n",
    "*   RC_50Hz: Fault currents\n",
    "*   RC_150Hz: Fault currents\n",
    "*   RC_<100Hz: Fault currents\n",
    "*   RC_100Hz-1kHz: Fault currents\n",
    "*   RC_>10kHz: Fault currents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "qIhc9bwK2926",
    "outputId": "fc7dd9cf-09e8-43e0-99d8-09da048c1f2c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  time       PLN1      PLN2      PLN3      ULL1      ULL2  \\\n0  2020-06-01 00:00:00  1141.0819  519.5034  482.9381  398.8613  400.1982   \n1  2020-06-01 00:01:00  1145.1162  519.1807  491.4436  398.6934  400.1579   \n2  2020-06-01 00:02:00  1140.9558  743.3837  484.9942  398.4367  400.1205   \n3  2020-06-01 00:03:00  1151.9409  741.4836  487.4224  398.9800  400.4375   \n4  2020-06-01 00:04:00  1142.1594  741.9858  486.7629  398.7133  400.3145   \n\n       ULL3  COS_PHI1  COS_PHI2  COS_PHI3     FREQ  RC_DC  RC_AC  RC_50Hz  \\\n0  395.6010    0.8091    0.6864    0.4875  49.9927    4.0   91.0     10.0   \n1  395.5431    0.8080    0.6903    0.4904  49.9779    5.0   64.0      7.0   \n2  395.5259    0.8113    0.9274    0.4806  49.9782    4.0   64.0      7.0   \n3  395.8621    0.8249    0.9123    0.4778  49.9850    5.0   66.0      8.0   \n4  395.6446    0.8081    0.9291    0.4552  49.9856    4.0   85.0     11.0   \n\n   RC_150Hz  RC_<100Hz  RC_100Hz-1kHz  RC_>1kHz  RC_>10kHz  \n0      39.0       36.0           86.0      82.0        7.0  \n1      27.0       25.0           60.0      55.0        2.0  \n2      27.0       25.0           60.0      55.0        2.0  \n3      28.0       25.0           61.0      57.0        2.0  \n4      45.0       41.0           75.0      68.0        6.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>PLN1</th>\n      <th>PLN2</th>\n      <th>PLN3</th>\n      <th>ULL1</th>\n      <th>ULL2</th>\n      <th>ULL3</th>\n      <th>COS_PHI1</th>\n      <th>COS_PHI2</th>\n      <th>COS_PHI3</th>\n      <th>FREQ</th>\n      <th>RC_DC</th>\n      <th>RC_AC</th>\n      <th>RC_50Hz</th>\n      <th>RC_150Hz</th>\n      <th>RC_&lt;100Hz</th>\n      <th>RC_100Hz-1kHz</th>\n      <th>RC_&gt;1kHz</th>\n      <th>RC_&gt;10kHz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-06-01 00:00:00</td>\n      <td>1141.0819</td>\n      <td>519.5034</td>\n      <td>482.9381</td>\n      <td>398.8613</td>\n      <td>400.1982</td>\n      <td>395.6010</td>\n      <td>0.8091</td>\n      <td>0.6864</td>\n      <td>0.4875</td>\n      <td>49.9927</td>\n      <td>4.0</td>\n      <td>91.0</td>\n      <td>10.0</td>\n      <td>39.0</td>\n      <td>36.0</td>\n      <td>86.0</td>\n      <td>82.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-06-01 00:01:00</td>\n      <td>1145.1162</td>\n      <td>519.1807</td>\n      <td>491.4436</td>\n      <td>398.6934</td>\n      <td>400.1579</td>\n      <td>395.5431</td>\n      <td>0.8080</td>\n      <td>0.6903</td>\n      <td>0.4904</td>\n      <td>49.9779</td>\n      <td>5.0</td>\n      <td>64.0</td>\n      <td>7.0</td>\n      <td>27.0</td>\n      <td>25.0</td>\n      <td>60.0</td>\n      <td>55.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-06-01 00:02:00</td>\n      <td>1140.9558</td>\n      <td>743.3837</td>\n      <td>484.9942</td>\n      <td>398.4367</td>\n      <td>400.1205</td>\n      <td>395.5259</td>\n      <td>0.8113</td>\n      <td>0.9274</td>\n      <td>0.4806</td>\n      <td>49.9782</td>\n      <td>4.0</td>\n      <td>64.0</td>\n      <td>7.0</td>\n      <td>27.0</td>\n      <td>25.0</td>\n      <td>60.0</td>\n      <td>55.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-06-01 00:03:00</td>\n      <td>1151.9409</td>\n      <td>741.4836</td>\n      <td>487.4224</td>\n      <td>398.9800</td>\n      <td>400.4375</td>\n      <td>395.8621</td>\n      <td>0.8249</td>\n      <td>0.9123</td>\n      <td>0.4778</td>\n      <td>49.9850</td>\n      <td>5.0</td>\n      <td>66.0</td>\n      <td>8.0</td>\n      <td>28.0</td>\n      <td>25.0</td>\n      <td>61.0</td>\n      <td>57.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-06-01 00:04:00</td>\n      <td>1142.1594</td>\n      <td>741.9858</td>\n      <td>486.7629</td>\n      <td>398.7133</td>\n      <td>400.3145</td>\n      <td>395.6446</td>\n      <td>0.8081</td>\n      <td>0.9291</td>\n      <td>0.4552</td>\n      <td>49.9856</td>\n      <td>4.0</td>\n      <td>85.0</td>\n      <td>11.0</td>\n      <td>45.0</td>\n      <td>41.0</td>\n      <td>75.0</td>\n      <td>68.0</td>\n      <td>6.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data_factory.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZAyQ-cA2926"
   },
   "source": [
    "Once we have the dataset, we should prepare it. Finding the missing or the NaN values and replace them with suitable values (in this case we use the previous value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNTHq6mO2927",
    "outputId": "7fc4c0ed-8bdd-4914-b684-aa387a355e3d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values before cleaning: 2546\n",
      "Number of rows with NaN values after cleaning: 0\n",
      "Total number of samples: 63360\n",
      "Number of features: 19\n"
     ]
    }
   ],
   "source": [
    "# Replace all mising values with NaN\n",
    "dataset = dataset.replace(' ', np.nan)\n",
    "# Search for all the rows with NaN values\n",
    "nan_values = dataset[dataset.isna().any(axis=1)]\n",
    "# Print the shape to know how many are there\n",
    "print(f'Number of rows with NaN values before cleaning: {nan_values.shape[0]}') \n",
    "\n",
    "# Fill all NaN values with the previous row value\n",
    "dataset_clean = dataset.fillna(method='ffill')\n",
    "\n",
    "# Check that there isn't any NaN values\n",
    "nan_values = dataset_clean[dataset_clean.isna().any(axis=1)]\n",
    "# Print the shape to know how many are there\n",
    "print(f'Number of rows with NaN values after cleaning: {nan_values.shape[0]}') \n",
    "\n",
    "#Total number of samples\n",
    "print(f'Total number of samples: {dataset_clean.shape[0]}')\n",
    "print(f'Number of features: {dataset_clean.shape[1]}')\n",
    "\n",
    "# Set to True to print the graphs\n",
    "print_graphs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6R7YF-T2928"
   },
   "source": [
    "Now we look at the distribution of the different features of the data over different time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tQ0vhNNv2928",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if print_graphs is True:\n",
    "\n",
    "    # PLN_1 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 1, 'PLN_1')\n",
    "\n",
    "    # PLN_1 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 1, 'PLN_1')\n",
    "\n",
    "    # PLN_2 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 2, 'PLN_2')\n",
    "\n",
    "    # PLN_2 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 2, 'PLN_2')\n",
    "\n",
    "    # PLN_3 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 3, 'PLN_3')\n",
    "\n",
    "    # PLN_3 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 3, 'PLN_3')\n",
    "\n",
    "    # ULL1 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 4, 'ULL1')\n",
    "\n",
    "    # ULL1 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 4, 'ULL1')\n",
    "\n",
    "    # ULL2 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 5, 'ULL2')\n",
    "\n",
    "    # ULL2 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 5, 'ULL2')\n",
    "\n",
    "    # ULL3 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 6, 'ULL3')\n",
    "\n",
    "    # ULL3 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 6, 'ULL3')\n",
    "\n",
    "    # COS_PHI1 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 7, 'COS_PHI1')\n",
    "\n",
    "    # COS_PHI1 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 7, 'COS_PHI1')\n",
    "\n",
    "    # COS_PHI2 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 8, 'COS_PHI2')\n",
    "\n",
    "    # COS_PHI2 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 8, 'COS_PHI2')\n",
    "\n",
    "    # COS_PHI3 in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 9, 'COS_PHI3')\n",
    "\n",
    "    # COS_PHI3 in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 9, 'COS_PHI3')\n",
    "\n",
    "    # FREQ in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 10, 'FREQ')\n",
    "\n",
    "    # FREQ in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 10, 'FREQ')\n",
    "\n",
    "    # RC_DC in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 11, 'RC_DC')\n",
    "\n",
    "    # RC_DC in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 11, 'RC_DC')\n",
    "\n",
    "    # RC_AC in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 12, 'RC_AC')\n",
    "\n",
    "    # RC_AC in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 12, 'RC_AC')\n",
    "\n",
    "    # RC_50Hz in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 13, 'RC_50Hz')\n",
    "\n",
    "    # RC_50Hz in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 13, 'RC_50Hz')\n",
    "\n",
    "    # RC_150Hz in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 14, 'RC_150Hz')\n",
    "\n",
    "    # RC_150Hz in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 14, 'RC_150Hz')\n",
    "\n",
    "    # RC_100Hz_1kHz in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 15, 'RC_100Hz_1kHz')\n",
    "\n",
    "    # RC_100Hz_1kHz in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 15, 'RC_100Hz_1kHz')\n",
    "\n",
    "    # RC_100Hz_1kHz in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 16, 'RC_100Hz_1kHz')\n",
    "\n",
    "    # RC_100Hz_1kHz in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 16, 'RC_100Hz_1kHz')\n",
    "\n",
    "    # RC_more_1kHz in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 17, 'RC_more_1kHz')\n",
    "\n",
    "    # RC_more_1kHz in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 17, 'RC_more_1kHz')\n",
    "\n",
    "    # RC_more_10kHz in a weekly interval\n",
    "\n",
    "    utils_bsc.week_plot(dataset_clean, 18, 'RC_more_10kHz')\n",
    "\n",
    "    # RC_more_10kHz in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
    "\n",
    "    utils_bsc.daily_plot(dataset_clean, 18, 'RC_more_10kHz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation Training and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvocvIBA292-"
   },
   "source": [
    "Once the dataset is prepared, make batches of data,put them togheter in an array and split them into train and test sets.\n",
    "After looking through the dataset and the features, i decided to takeonly the values with a timestap of a weekday between 4:00 and 19:30. In many of the features in the interval otside those timestamps there i only noise, which can be a sign that the machine is off in that time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M0CY804_292-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27840 sequences of longitud 60 with 18 features\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([278, 60, 18])\n",
      "tensor([[1.4142e+04, 8.0785e+03, 6.8398e+03,  ..., 9.5000e+01, 9.1000e+01,\n",
      "         4.0000e+00],\n",
      "        [1.4374e+04, 9.1460e+03, 7.3355e+03,  ..., 8.6000e+01, 8.1000e+01,\n",
      "         3.0000e+00],\n",
      "        [1.4541e+04, 8.7673e+03, 7.1638e+03,  ..., 7.5000e+01, 6.2000e+01,\n",
      "         3.0000e+00],\n",
      "        ...,\n",
      "        [1.1810e+04, 6.8767e+03, 7.2053e+03,  ..., 8.6000e+01, 8.3000e+01,\n",
      "         3.0000e+00],\n",
      "        [1.1766e+04, 8.5856e+03, 8.3816e+03,  ..., 8.0000e+01, 7.6000e+01,\n",
      "         3.0000e+00],\n",
      "        [1.2599e+04, 9.5235e+03, 6.5753e+03,  ..., 8.0000e+01, 7.5000e+01,\n",
      "         3.0000e+00]])\n",
      "length of training set: 278\n",
      "length of test set: 27562\n"
     ]
    }
   ],
   "source": [
    "# Create 63300 batches of longitud 60\n",
    "\n",
    "endset = utils_bsc.create_batches(dataset_clean, 60)\n",
    "    \n",
    "print(f'{len(endset)} sequences of longitud {endset[0].shape[0]} with {endset[0].shape[1]} features')\n",
    "\n",
    "# Spliting into train and test sets\n",
    "\n",
    "training_data, testing_data = train_test_split(endset, test_size=0.99, random_state=25)\n",
    "\n",
    "train_set = torch.stack(training_data).float()\n",
    "test_set = torch.stack(testing_data).float()\n",
    "\n",
    "print(type(train_set))\n",
    "print(train_set.shape)\n",
    "print(train_set[0])\n",
    "\n",
    "print(f'length of training set: {train_set.shape[0]}')\n",
    "print(f'length of test set: {test_set.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dUCsWWo292_"
   },
   "source": [
    "Now, we define a class with the transformer model that we are going to use:\n",
    "\n",
    "Using the already written pytorch library for Transformers:\n",
    "\n",
    "1) torch.nn.TransformerEncoderLayer\n",
    "\n",
    "*   d_model –> the number of expected features in the input (required).\n",
    "*   nhead –> the number of heads in the multiheadattention models (required).\n",
    "*   dropout –> the dropout value (default=0.1).\n",
    "*   activation –> the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. (default: relu)\n",
    "*   layer_norm_eps –> the eps value in layer normalization components (default=1e-5).\n",
    "*   batch_first –> If True, then the input and output tensors are provided as (batch, seq, feature). (default: False)\n",
    "*   norm_first –> if True, layer norm is done prior to attention and feedforward operations, respectivaly. Otherwise it’s done after. (default: False (after))\n",
    "\n",
    "2) torch.nn.TransformerDecoderLayer\n",
    "\n",
    "* d_model –> the number of expected features in the input (required).\n",
    "* nhead –> the number of heads in the multiheadattention models (required).\n",
    "* dim_feedforward –> the dimension of the feedforward network model (default=2048).\n",
    "* dropout –> the dropout value (default=0.1).\n",
    "* activation –> the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu\n",
    "* layer_norm_eps –> the eps value in layer normalization components (default=1e-5).\n",
    "* batch_first –> If True, then the input and output tensors are provided as (batch, seq, feature). Default: False.\n",
    "* norm_first –> if True, layer norm is done prior to self attention, multihead attention and feedforward operations, respectivaly. Otherwise it’s done after. Default: False (after).\n",
    "\n",
    "3) torch.nn.TransformerEncoder\n",
    "\n",
    "* encoder_layer –> an instance of the TransformerEncoderLayer() class (required).\n",
    "* num_layers –> the number of sub-encoder-layers in the encoder (required).\n",
    "* norm –> the layer normalization component (optional).\n",
    "\n",
    "\n",
    "4) torch.nn.TransformerDecoder\n",
    "\n",
    "* decoder_layer – an instance of the TransformerDecoderLayer() class (required).\n",
    "* num_layers – the number of sub-decoder-layers in the decoder (required).\n",
    "* norm – the layer normalization component (optional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_encoding\n",
      "layer\n",
      "tensor([[[ 2.8976,  1.7337,  1.5571,  ..., -0.5176, -0.4087, -1.1886],\n",
      "         [ 2.9014,  1.9131,  1.3244,  ..., -0.4623, -0.2406, -1.2524],\n",
      "         [ 2.9072,  1.7828,  1.5040,  ..., -0.4886, -0.3798, -1.1882],\n",
      "         ...,\n",
      "         [ 2.6312,  1.8613,  1.7960,  ..., -0.3809, -0.3168, -1.1287],\n",
      "         [ 2.3779,  2.0586,  1.8912,  ..., -0.3669, -0.3578, -1.1861],\n",
      "         [ 2.6980,  2.2626,  1.1491,  ..., -0.3965, -0.3481, -1.1454]],\n",
      "\n",
      "        [[ 2.7889,  1.8631,  1.5048,  ..., -0.4660, -0.3809, -1.2218],\n",
      "         [ 2.7075,  1.9056,  1.6957,  ..., -0.3819, -0.3784, -1.2086],\n",
      "         [ 2.9020,  1.7404,  1.5643,  ..., -0.4591, -0.3735, -1.1327],\n",
      "         ...,\n",
      "         [ 2.9783,  1.7347,  1.4021,  ..., -0.4265, -0.2964, -1.1498],\n",
      "         [ 2.7351,  1.8103,  1.7730,  ..., -0.7394, -0.3158, -1.1176],\n",
      "         [ 2.8132,  1.6054,  1.9010,  ..., -0.4709, -0.3453, -0.5232]],\n",
      "\n",
      "        [[ 2.3948,  2.2421,  1.7357,  ..., -0.4068, -0.4130, -1.1587],\n",
      "         [ 2.6440,  2.1558,  1.5161,  ..., -0.4691, -0.3830, -1.0838],\n",
      "         [ 2.5900,  2.1113,  1.6868,  ..., -0.4201, -0.4227, -1.0570],\n",
      "         ...,\n",
      "         [ 2.3810,  2.0730,  1.8242,  ..., -0.7199, -0.3417, -1.1574],\n",
      "         [ 2.5388,  1.9805,  1.9333,  ..., -0.4682, -0.5077, -0.5055],\n",
      "         [ 2.4978,  2.0024,  1.8436,  ..., -0.3446, -0.3836, -1.1417]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4753,  2.0542,  1.9356,  ..., -0.4197, -0.3747, -1.0842],\n",
      "         [ 2.8131,  1.7626,  1.6147,  ..., -0.4800, -0.3545, -1.2799],\n",
      "         [ 2.9387,  1.6915,  1.5131,  ..., -0.4908, -0.3287, -1.1486],\n",
      "         ...,\n",
      "         [ 2.6403,  1.7370,  1.8956,  ..., -0.3945, -0.3330, -1.1531],\n",
      "         [ 2.7882,  1.9896,  1.4107,  ..., -0.7476, -0.2855, -0.9952],\n",
      "         [ 2.7391,  1.7912,  1.8755,  ..., -0.4247, -0.4409, -0.4727]],\n",
      "\n",
      "        [[ 2.9821,  1.6802,  1.4174,  ..., -0.4931, -0.3814, -1.2319],\n",
      "         [ 2.3272,  2.3745,  1.6036,  ..., -0.3791, -0.3826, -1.1513],\n",
      "         [ 2.4818,  2.0937,  1.7701,  ..., -0.4054, -0.4035, -1.2942],\n",
      "         ...,\n",
      "         [ 2.5352,  1.9583,  1.7615,  ..., -0.5270, -0.3064, -1.2112],\n",
      "         [ 3.1260,  1.4805,  1.3523,  ..., -0.5616, -0.3396, -1.1508],\n",
      "         [ 2.9118,  1.6661,  1.4677,  ..., -0.5239, -0.3666, -1.2776]],\n",
      "\n",
      "        [[ 2.9846,  1.7803,  1.2700,  ..., -0.0260, -0.4394, -1.2975],\n",
      "         [ 2.6432,  2.1439,  1.4421,  ..., -0.4204, -0.3667, -1.2374],\n",
      "         [ 2.9072,  1.4303,  1.8052,  ..., -0.5632, -0.4000, -1.1299],\n",
      "         ...,\n",
      "         [ 2.6015,  1.7388,  1.7925,  ..., -0.3360, -0.2997, -1.2980],\n",
      "         [ 2.5206,  1.8875,  1.9430,  ..., -0.4003, -0.3683, -1.2227],\n",
      "         [ 2.3052,  2.3603,  1.5991,  ..., -0.3985, -0.3871, -1.2192]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Transformer Encoder\n",
    "\n",
    "model = Transformer (num_encoder_layers=1,\n",
    "                    num_decoder_layers=1,\n",
    "                    dim_model=18,\n",
    "                    num_encoder_heads=1,\n",
    "                    units_hidden_layer=2048,\n",
    "                    dropout=0.1,\n",
    "                    activation=nn.ReLU(),\n",
    "                    mask=True,\n",
    "                    device=device(\"cpu\"))\n",
    "\n",
    "lst = [1, 3, 5, 7, 11]\n",
    "\n",
    "for element in tqdm(lst):\n",
    "    sleep(0.1)\n",
    "\n",
    "out = model(train_set)\n",
    "\n",
    "print(out)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tCC_Bava293A",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, feature_size, output_size, num_encoder_layers, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model= feature_size, nhead= feature_size, dropout= dropout)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model= feature_size, nhead= feature_size, dropout= dropout)\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers= num_encoder_layers)\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers= num_encoder_layers)\n",
    "\n",
    "    def generate_square_mask(self, dim):\n",
    "        return torch.triu(torch.ones(dim, dim) * float('-inf'), diagonal=1)\n",
    "        \n",
    "    def forward (self, src, device):\n",
    "        print(len(src))\n",
    "        mask = self.generate_square_mask(len(src))\n",
    "        print('mask')\n",
    "        output = self.encoder (src, mask)\n",
    "        print('encoder')\n",
    "        output = self.decoder (output, 18)\n",
    "        print('decoder')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FgrbFBLS293A",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "mask\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/2s/vg97_p793_97vgjwtr42qdm80000gn/T/ipykernel_56474/1947465165.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtransformer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTransformer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m18\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m18\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtransformer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cpu\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/2s/vg97_p793_97vgjwtr42qdm80000gn/T/ipykernel_56474/1616670650.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, device)\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mmask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate_square_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'mask'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoder\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'encoder'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecoder\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m18\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, mask, src_key_padding_mask)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmod\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 198\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, src_mask, src_key_padding_mask)\u001B[0m\n\u001B[1;32m    337\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ff_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    338\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 339\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sa_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    340\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ff_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    341\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001B[0m in \u001B[0;36m_sa_block\u001B[0;34m(self, x, attn_mask, key_padding_mask)\u001B[0m\n\u001B[1;32m    345\u001B[0m     def _sa_block(self, x: Tensor,\n\u001B[1;32m    346\u001B[0m                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n\u001B[0;32m--> 347\u001B[0;31m         x = self.self_attn(x, x, x,\n\u001B[0m\u001B[1;32m    348\u001B[0m                            \u001B[0mattn_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattn_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    349\u001B[0m                            \u001B[0mkey_padding_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkey_padding_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/activation.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001B[0m\n\u001B[1;32m   1001\u001B[0m                 v_proj_weight=self.v_proj_weight)\n\u001B[1;32m   1002\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1003\u001B[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001B[0m\u001B[1;32m   1004\u001B[0m                 \u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membed_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_heads\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1005\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_proj_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_proj_bias\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mmulti_head_attention_forward\u001B[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001B[0m\n\u001B[1;32m   5099\u001B[0m     \u001B[0;31m# (deep breath) calculate attention and out projection\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5100\u001B[0m     \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5101\u001B[0;31m     \u001B[0mattn_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_output_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_scaled_dot_product_attention\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdropout_p\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5102\u001B[0m     \u001B[0mattn_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattn_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtgt_len\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbsz\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membed_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5103\u001B[0m     \u001B[0mattn_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_proj_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_proj_bias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36m_scaled_dot_product_attention\u001B[0;34m(q, k, v, attn_mask, dropout_p)\u001B[0m\n\u001B[1;32m   4842\u001B[0m     \u001B[0mq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mq\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4843\u001B[0m     \u001B[0;31m# (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4844\u001B[0;31m     \u001B[0mattn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbmm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4845\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mattn_mask\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4846\u001B[0m         \u001B[0mattn\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mattn_mask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "transformer = Transformer(18, 18, 4, 0)\n",
    "\n",
    "transformer.forward(train_set, device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS8DcLC-293B"
   },
   "source": [
    "Ideas, things to remember, to search, etc...\n",
    "\n",
    "reconstruction, vergelich mit base line model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "bsc_arbeit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python392jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}