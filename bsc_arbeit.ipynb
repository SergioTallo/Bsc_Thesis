{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0esqvQHT2922"
      },
      "source": [
        "# First: load imports needed for the project and preparation of the project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is necessary to use this notebook in google colab\n",
        "# If you are running this notebook in colab, please change colab to True\n",
        "\n",
        "import os\n",
        "\n",
        "colab = True\n",
        "cwd = os.getcwd()\n",
        "\n",
        "if colab is True and cwd != \"/content/Bsc_Thesis\":\n",
        "  ! git clone https://github.com/SergioTallo/Bsc_Thesis.git\n",
        "  % cd Bsc_Thesis\n",
        "\n",
        "print(cwd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adt0VN_ojbV1",
        "outputId": "cdc946b8-b8c7-4211-9036-9eca665d3203"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bsc_Thesis'...\n",
            "remote: Enumerating objects: 203, done.\u001b[K\n",
            "remote: Counting objects: 100% (203/203), done.\u001b[K\n",
            "remote: Compressing objects: 100% (191/191), done.\u001b[K\n",
            "remote: Total 203 (delta 125), reused 26 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (203/203), 3.89 MiB | 8.43 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n",
            "/content/Bsc_Thesis\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VCwEuYFk2923",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855e18d8-b814-4c6f-f033-8ebfcd821f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: GPU = Tesla P100-PCIE-16GB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import Tensor, float32, sin, cos\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import utils_bsc\n",
        "import datetime\n",
        "import statistics\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "  print('Device: GPU =', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('Device: CPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQPvc4I-4LY5",
        "outputId": "21e3f4df-b2bc-4aff-e235-93858d1eec0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "versions of packages:\n",
            "Python: 3.7.13\n",
            "Pandas: 1.3.5\n",
            "Numpy: 1.21.5\n",
            "PyTorch: 1.10.0+cu111\n",
            "Sklearn: 1.0.2\n",
            "seaborn: 0.11.2\n"
          ]
        }
      ],
      "source": [
        "utils_bsc.print_versions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICcfWNfajDhn"
      },
      "source": [
        "# Data loading and preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "bjUFrMX32925"
      },
      "source": [
        "Now, we should create a dataset with all the data stored in the .csv file\n",
        "\n",
        "Description of the data:\n",
        "\n",
        "*   time: Timestamp (YYYY-MM-DD HH:MM:SS)\n",
        "*   PLN1: Power in the phase 1 (W)\n",
        "*   PLN2: Power in the phase 2 (W)\n",
        "*   PLN3: Power in the phase 3 (W)\n",
        "*   ULL1: Current Voltage between 2 phases (V)\n",
        "*   ULL2: Current Voltage between 2 phases (V)\n",
        "*   ULL3: Current Voltage between 2 phases (V)\n",
        "*   COS_PHI1: Phase shift (Cos)\n",
        "*   COS_PHI2: Phase shift (Cos)\n",
        "*   COS_PHI3: Phase shift (Cos)\n",
        "*   FREQ: Electricity Frequency (Hz)\n",
        "*   RC_DC: Fault currents\n",
        "*   RC_AC: Fault currents\n",
        "*   RC_50Hz: Fault currents\n",
        "*   RC_150Hz: Fault currents\n",
        "*   RC_<100Hz: Fault currents\n",
        "*   RC_100Hz-1kHz: Fault currents\n",
        "*   RC_>10kHz: Fault currents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "qIhc9bwK2926",
        "outputId": "cc1a64a9-4812-44bb-afe0-d772a718d8c7",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  time       PLN1      PLN2      PLN3      ULL1      ULL2  \\\n",
              "0  2020-06-01 00:00:00  1141.0819  519.5034  482.9381  398.8613  400.1982   \n",
              "1  2020-06-01 00:01:00  1145.1162  519.1807  491.4436  398.6934  400.1579   \n",
              "2  2020-06-01 00:02:00  1140.9558  743.3837  484.9942  398.4367  400.1205   \n",
              "3  2020-06-01 00:03:00  1151.9409  741.4836  487.4224  398.9800  400.4375   \n",
              "4  2020-06-01 00:04:00  1142.1594  741.9858  486.7629  398.7133  400.3145   \n",
              "\n",
              "       ULL3  COS_PHI1  COS_PHI2  COS_PHI3     FREQ  RC_DC  RC_AC  RC_50Hz  \\\n",
              "0  395.6010    0.8091    0.6864    0.4875  49.9927    4.0   91.0     10.0   \n",
              "1  395.5431    0.8080    0.6903    0.4904  49.9779    5.0   64.0      7.0   \n",
              "2  395.5259    0.8113    0.9274    0.4806  49.9782    4.0   64.0      7.0   \n",
              "3  395.8621    0.8249    0.9123    0.4778  49.9850    5.0   66.0      8.0   \n",
              "4  395.6446    0.8081    0.9291    0.4552  49.9856    4.0   85.0     11.0   \n",
              "\n",
              "   RC_150Hz  RC_<100Hz  RC_100Hz-1kHz  RC_>1kHz  RC_>10kHz  \n",
              "0      39.0       36.0           86.0      82.0        7.0  \n",
              "1      27.0       25.0           60.0      55.0        2.0  \n",
              "2      27.0       25.0           60.0      55.0        2.0  \n",
              "3      28.0       25.0           61.0      57.0        2.0  \n",
              "4      45.0       41.0           75.0      68.0        6.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d871f928-7c98-46d3-910c-c0798a313ddc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>PLN1</th>\n",
              "      <th>PLN2</th>\n",
              "      <th>PLN3</th>\n",
              "      <th>ULL1</th>\n",
              "      <th>ULL2</th>\n",
              "      <th>ULL3</th>\n",
              "      <th>COS_PHI1</th>\n",
              "      <th>COS_PHI2</th>\n",
              "      <th>COS_PHI3</th>\n",
              "      <th>FREQ</th>\n",
              "      <th>RC_DC</th>\n",
              "      <th>RC_AC</th>\n",
              "      <th>RC_50Hz</th>\n",
              "      <th>RC_150Hz</th>\n",
              "      <th>RC_&lt;100Hz</th>\n",
              "      <th>RC_100Hz-1kHz</th>\n",
              "      <th>RC_&gt;1kHz</th>\n",
              "      <th>RC_&gt;10kHz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-06-01 00:00:00</td>\n",
              "      <td>1141.0819</td>\n",
              "      <td>519.5034</td>\n",
              "      <td>482.9381</td>\n",
              "      <td>398.8613</td>\n",
              "      <td>400.1982</td>\n",
              "      <td>395.6010</td>\n",
              "      <td>0.8091</td>\n",
              "      <td>0.6864</td>\n",
              "      <td>0.4875</td>\n",
              "      <td>49.9927</td>\n",
              "      <td>4.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-06-01 00:01:00</td>\n",
              "      <td>1145.1162</td>\n",
              "      <td>519.1807</td>\n",
              "      <td>491.4436</td>\n",
              "      <td>398.6934</td>\n",
              "      <td>400.1579</td>\n",
              "      <td>395.5431</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6903</td>\n",
              "      <td>0.4904</td>\n",
              "      <td>49.9779</td>\n",
              "      <td>5.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-06-01 00:02:00</td>\n",
              "      <td>1140.9558</td>\n",
              "      <td>743.3837</td>\n",
              "      <td>484.9942</td>\n",
              "      <td>398.4367</td>\n",
              "      <td>400.1205</td>\n",
              "      <td>395.5259</td>\n",
              "      <td>0.8113</td>\n",
              "      <td>0.9274</td>\n",
              "      <td>0.4806</td>\n",
              "      <td>49.9782</td>\n",
              "      <td>4.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-06-01 00:03:00</td>\n",
              "      <td>1151.9409</td>\n",
              "      <td>741.4836</td>\n",
              "      <td>487.4224</td>\n",
              "      <td>398.9800</td>\n",
              "      <td>400.4375</td>\n",
              "      <td>395.8621</td>\n",
              "      <td>0.8249</td>\n",
              "      <td>0.9123</td>\n",
              "      <td>0.4778</td>\n",
              "      <td>49.9850</td>\n",
              "      <td>5.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-06-01 00:04:00</td>\n",
              "      <td>1142.1594</td>\n",
              "      <td>741.9858</td>\n",
              "      <td>486.7629</td>\n",
              "      <td>398.7133</td>\n",
              "      <td>400.3145</td>\n",
              "      <td>395.6446</td>\n",
              "      <td>0.8081</td>\n",
              "      <td>0.9291</td>\n",
              "      <td>0.4552</td>\n",
              "      <td>49.9856</td>\n",
              "      <td>4.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d871f928-7c98-46d3-910c-c0798a313ddc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d871f928-7c98-46d3-910c-c0798a313ddc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d871f928-7c98-46d3-910c-c0798a313ddc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataset = pd.read_csv('data_factory.csv')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZAyQ-cA2926"
      },
      "source": [
        "Once we have the dataset, we should prepare it. Finding the missing or the NaN values and replace them with suitable values (in this case we use the previous value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNTHq6mO2927",
        "outputId": "eef4242b-3bf6-47dd-f44d-a1ff771fda37",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with NaN values before cleaning: 2546\n",
            "Number of rows with NaN values after cleaning: 0\n",
            "Total number of samples: 63360\n",
            "Number of features: 19\n"
          ]
        }
      ],
      "source": [
        "# Replace all mising values with NaN\n",
        "dataset = dataset.replace(' ', np.nan)\n",
        "# Search for all the rows with NaN values\n",
        "nan_values = dataset[dataset.isna().any(axis=1)]\n",
        "# Print the shape to know how many are there\n",
        "print(f'Number of rows with NaN values before cleaning: {nan_values.shape[0]}') \n",
        "\n",
        "# Fill all NaN values with the previous row value\n",
        "dataset_clean = dataset.fillna(method='ffill')\n",
        "\n",
        "# Check that there isn't any NaN values\n",
        "nan_values = dataset_clean[dataset_clean.isna().any(axis=1)]\n",
        "# Print the shape to know how many are there\n",
        "print(f'Number of rows with NaN values after cleaning: {nan_values.shape[0]}') \n",
        "\n",
        "#Total number of samples\n",
        "print(f'Total number of samples: {dataset_clean.shape[0]}')\n",
        "print(f'Number of features: {dataset_clean.shape[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d44xLGPbjDhp"
      },
      "source": [
        "# Distribution of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6R7YF-T2928"
      },
      "source": [
        "Now we look at the distribution of the different features of the data over different time intervals.\n",
        "First we take a look of the min and max values, mean and median value and the standard deviation of every feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_data = False\n",
        "\n",
        "if print_data is True:\n",
        "  for column in dataset_clean.columns:\n",
        "    if column == 'time':\n",
        "      print(column)\n",
        "      print('Min value: ', dataset_clean[column].min())\n",
        "      print('Max value: ', dataset_clean[column].max())\n",
        "      print('')\n",
        "    else:\n",
        "      print(column)\n",
        "      print('Min value: ', dataset_clean[column].min())\n",
        "      print('Max value: ', dataset_clean[column].max())\n",
        "      print('Mean value: ', dataset_clean[column].mean())\n",
        "      print('Median value: ', dataset_clean[column].median())\n",
        "      print('Standard deviation: ', dataset_clean[column].std())\n",
        "      print('')"
      ],
      "metadata": {
        "id": "pseEB_3qChk4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tQ0vhNNv2928",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Set to True to print the graphs\n",
        "\n",
        "print_graphs = False\n",
        "\n",
        "if print_graphs is True:\n",
        "\n",
        "  for i, column in enumerate(dataset_clean.columns):\n",
        "    if i > 0:\n",
        "      # Feature in a weekly interval\n",
        "      utils_bsc.week_plot(dataset_clean, i, column)\n",
        "      # Feature in a daily interval (only the values of weekdays between 4:00 and 19:30)\n",
        "      utils_bsc.daily_plot(dataset_clean, i, column)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We print some graphs showing the density distribution of every feature\n",
        "\n",
        "print_graphs = False\n",
        "\n",
        "if print_graphs is True:\n",
        "  for column in tqdm(dataset_clean.columns):\n",
        "    if column != 'time':\n",
        "      sns.displot(dataset_clean, x=column, kind=\"kde\")"
      ],
      "metadata": {
        "id": "sJIFPsqkiezx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After looking to the different data graphs i notice there two very different \"time slots\" when the data differs. One is Weekdays between 4:00 and 19:30. The other is Weekdays bewteen 19:30 and 4:00 and Weekends."
      ],
      "metadata": {
        "id": "cWFCmrIH5oA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create two extra data sets, one with the weekdays between 4:00 and 18:30 and one with the rest.\n",
        "dataset_clean_time = pd.to_datetime(dataset_clean['time'])\n",
        "\n",
        "day_mask = dataset_clean_time.dt.day_name()\n",
        "\n",
        "time_mask = (dataset_clean_time.dt.hour >= 4) & ((dataset_clean_time.dt.hour < 19) | ((dataset_clean_time.dt.hour == 19) & (dataset_clean_time.dt.minute <= 30))) & ((day_mask == ('Monday')) | (day_mask == ('Tuesday')) | (day_mask == ('Wednesday')) | (day_mask == ('Thursday')) | (day_mask == ('Friday')))\n",
        "\n",
        "dataset_weekdays = dataset_clean[time_mask]\n",
        "\n",
        "for i in range(len(time_mask)):\n",
        "  if time_mask[i] == False:\n",
        "    time_mask[i] = True\n",
        "  elif time_mask[i] == True:\n",
        "    time_mask[i] = False\n",
        "\n",
        "dataset_weekend = dataset_clean[time_mask]\n",
        "\n",
        "print(f'Weekdays dataset size: {len(dataset_weekdays)}')\n",
        "print(f'Weekend dataset size: {len(dataset_weekend)}')"
      ],
      "metadata": {
        "id": "hVOHl2YDmFV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d48692-c206-4723-a5bc-e5005a9171ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weekdays dataset size: 29792\n",
            "Weekend dataset size: 33568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_graphs = False\n",
        "\n",
        "if print_graphs is True:\n",
        "  for column in tqdm(dataset_weekdays.columns):\n",
        "    if column != 'time':\n",
        "      sns.displot(dataset_weekdays, x=column, kind=\"kde\")"
      ],
      "metadata": {
        "id": "jYbGTF6mSz5v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_graphs = False\n",
        "\n",
        "if print_graphs is True:\n",
        "  for column in tqdm(dataset_weekend.columns):\n",
        "    if column != 'time':\n",
        "      sns.displot(dataset_weekend, x=column, kind=\"kde\")"
      ],
      "metadata": {
        "id": "hqupNsJw6bzH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this time we have three different datasets:\n",
        "\n",
        "* dataset_clean (Whole dataset)\n",
        "* dataset_weekdays (Entries from weekdays from 4:00 to 19:30)\n",
        "* dataset_weekend (Entries from Weekends and from weekdays from 19:30 to 4:00)\n",
        "\n"
      ],
      "metadata": {
        "id": "mgEydMmqTHtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset normalisation\n",
        "\n",
        "The scale of the data of the different features is very dofferent. Its better to have all of the features in the same scale. Therefore we perform a data normalisation. We choose to do a mean/stddev normalisation. We substract from every value the mean value of the feature and divide the result value by the std dev of this specific feature to have feature values with mean 0 and stddev of 1."
      ],
      "metadata": {
        "id": "B6iRPxmuJzVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the mean / stddev scaling in Pandas using the .mean() and .std() methods\n",
        "def normalize_mean_std_dataset(df):\n",
        "    # copy the dataframe\n",
        "    df_norm = df.copy()\n",
        "    # apply mean / stddev scaling\n",
        "    for column in tqdm(df_norm.columns):\n",
        "      if column != 'time':\n",
        "        df_norm[column] = (df_norm[column] - df_norm[column].mean()) / df_norm[column].std()\n",
        "    return df_norm"
      ],
      "metadata": {
        "id": "HBGfdNkAxxbN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the data normalisation in the whole dataset. We can print the distribution of the data if we want.\n",
        "dataset_norm = normalize_mean_std_dataset(dataset_clean)\n",
        "\n",
        "print_graphs = False\n",
        "\n",
        "if print_graphs is True:\n",
        "  for column in tqdm(dataset_norm.columns):\n",
        "    if column != 'time':\n",
        "      sns.displot(dataset_norm, x=column, kind=\"kde\")"
      ],
      "metadata": {
        "id": "1VhzsSn37b_0",
        "outputId": "c07d977e-d685-4a48-8d7a-51d00a874e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 723.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the data normalisation in the weekdays dataset. We can print the distribution of the data if we want.\n",
        "dataset_weekdays_norm = normalize_mean_std_dataset(dataset_weekdays)\n",
        "\n",
        "print_graphs = False\n",
        "\n",
        "if print_graphs is True:\n",
        "  for column in tqdm(dataset_weekdays_norm.columns):\n",
        "    if column != 'time':\n",
        "      sns.displot(dataset_weekdays_norm, x=column, kind=\"kde\")"
      ],
      "metadata": {
        "id": "SuS8dhouVCec",
        "outputId": "9734f361-fa16-4c3b-fa79-90709c765f06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 1142.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the data normalisation in the weekdays dataset. We can print the distribution of the data if we want.\n",
        "dataset_weekend_norm = normalize_mean_std_dataset(dataset_weekend)\n",
        "\n",
        "print_graphs = False\n",
        "\n",
        "if print_graphs is True:\n",
        "  for column in tqdm(dataset_weekend_norm.columns):\n",
        "    if column != 'time':\n",
        "      sns.displot(dataset_weekend_norm, x=column, kind=\"kde\")"
      ],
      "metadata": {
        "id": "MH07VtqpVdez",
        "outputId": "6ebb2879-51c3-4ed2-df12-90d954db45ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 923.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_norm.head()"
      ],
      "metadata": {
        "id": "FDUnKkascXyI",
        "outputId": "9b5362a2-5c3e-4898-c33b-63d8e8e6b130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  time      PLN1      PLN2      PLN3      ULL1      ULL2  \\\n",
              "0  2020-06-01 00:00:00 -1.075593 -1.045021 -1.051232  0.063478 -0.098312   \n",
              "1  2020-06-01 00:01:00 -1.074875 -1.045103 -1.048747  0.027004 -0.107515   \n",
              "2  2020-06-01 00:02:00 -1.075615 -0.988316 -1.050631 -0.028760 -0.116055   \n",
              "3  2020-06-01 00:03:00 -1.073661 -0.988798 -1.049922  0.089264 -0.043667   \n",
              "4  2020-06-01 00:04:00 -1.075401 -0.988670 -1.050114  0.031327 -0.071754   \n",
              "\n",
              "       ULL3  COS_PHI1  COS_PHI2  COS_PHI3      FREQ     RC_DC     RC_AC  \\\n",
              "0 -0.618908 -1.868350 -1.835847 -1.500292 -0.345935 -0.817380  0.632551   \n",
              "1 -0.632738 -1.884005 -1.803753 -1.486828 -1.139728  0.678985 -0.849829   \n",
              "2 -0.636846 -1.837041  0.147415 -1.532327 -1.123638 -0.817380 -0.849829   \n",
              "3 -0.556540 -1.643493  0.023152 -1.545327 -0.758922  0.678985 -0.740023   \n",
              "4 -0.608493 -1.882582  0.161405 -1.650254 -0.726741 -0.817380  0.303134   \n",
              "\n",
              "    RC_50Hz  RC_150Hz  RC_<100Hz  RC_100Hz-1kHz  RC_>1kHz  RC_>10kHz  \n",
              "0  1.075812  0.995360   1.143832       0.694697  0.747095   2.141318  \n",
              "1 -0.918340 -0.792166  -0.630653      -0.822036 -0.777047  -1.175568  \n",
              "2 -0.918340 -0.792166  -0.630653      -0.822036 -0.777047  -1.175568  \n",
              "3 -0.253623 -0.643206  -0.630653      -0.763700 -0.664147  -1.175568  \n",
              "4  1.740530  1.889123   1.950416       0.053002 -0.043201   1.477941  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc9609d6-7329-47b1-8598-0044ab27d240\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>PLN1</th>\n",
              "      <th>PLN2</th>\n",
              "      <th>PLN3</th>\n",
              "      <th>ULL1</th>\n",
              "      <th>ULL2</th>\n",
              "      <th>ULL3</th>\n",
              "      <th>COS_PHI1</th>\n",
              "      <th>COS_PHI2</th>\n",
              "      <th>COS_PHI3</th>\n",
              "      <th>FREQ</th>\n",
              "      <th>RC_DC</th>\n",
              "      <th>RC_AC</th>\n",
              "      <th>RC_50Hz</th>\n",
              "      <th>RC_150Hz</th>\n",
              "      <th>RC_&lt;100Hz</th>\n",
              "      <th>RC_100Hz-1kHz</th>\n",
              "      <th>RC_&gt;1kHz</th>\n",
              "      <th>RC_&gt;10kHz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-06-01 00:00:00</td>\n",
              "      <td>-1.075593</td>\n",
              "      <td>-1.045021</td>\n",
              "      <td>-1.051232</td>\n",
              "      <td>0.063478</td>\n",
              "      <td>-0.098312</td>\n",
              "      <td>-0.618908</td>\n",
              "      <td>-1.868350</td>\n",
              "      <td>-1.835847</td>\n",
              "      <td>-1.500292</td>\n",
              "      <td>-0.345935</td>\n",
              "      <td>-0.817380</td>\n",
              "      <td>0.632551</td>\n",
              "      <td>1.075812</td>\n",
              "      <td>0.995360</td>\n",
              "      <td>1.143832</td>\n",
              "      <td>0.694697</td>\n",
              "      <td>0.747095</td>\n",
              "      <td>2.141318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-06-01 00:01:00</td>\n",
              "      <td>-1.074875</td>\n",
              "      <td>-1.045103</td>\n",
              "      <td>-1.048747</td>\n",
              "      <td>0.027004</td>\n",
              "      <td>-0.107515</td>\n",
              "      <td>-0.632738</td>\n",
              "      <td>-1.884005</td>\n",
              "      <td>-1.803753</td>\n",
              "      <td>-1.486828</td>\n",
              "      <td>-1.139728</td>\n",
              "      <td>0.678985</td>\n",
              "      <td>-0.849829</td>\n",
              "      <td>-0.918340</td>\n",
              "      <td>-0.792166</td>\n",
              "      <td>-0.630653</td>\n",
              "      <td>-0.822036</td>\n",
              "      <td>-0.777047</td>\n",
              "      <td>-1.175568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-06-01 00:02:00</td>\n",
              "      <td>-1.075615</td>\n",
              "      <td>-0.988316</td>\n",
              "      <td>-1.050631</td>\n",
              "      <td>-0.028760</td>\n",
              "      <td>-0.116055</td>\n",
              "      <td>-0.636846</td>\n",
              "      <td>-1.837041</td>\n",
              "      <td>0.147415</td>\n",
              "      <td>-1.532327</td>\n",
              "      <td>-1.123638</td>\n",
              "      <td>-0.817380</td>\n",
              "      <td>-0.849829</td>\n",
              "      <td>-0.918340</td>\n",
              "      <td>-0.792166</td>\n",
              "      <td>-0.630653</td>\n",
              "      <td>-0.822036</td>\n",
              "      <td>-0.777047</td>\n",
              "      <td>-1.175568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-06-01 00:03:00</td>\n",
              "      <td>-1.073661</td>\n",
              "      <td>-0.988798</td>\n",
              "      <td>-1.049922</td>\n",
              "      <td>0.089264</td>\n",
              "      <td>-0.043667</td>\n",
              "      <td>-0.556540</td>\n",
              "      <td>-1.643493</td>\n",
              "      <td>0.023152</td>\n",
              "      <td>-1.545327</td>\n",
              "      <td>-0.758922</td>\n",
              "      <td>0.678985</td>\n",
              "      <td>-0.740023</td>\n",
              "      <td>-0.253623</td>\n",
              "      <td>-0.643206</td>\n",
              "      <td>-0.630653</td>\n",
              "      <td>-0.763700</td>\n",
              "      <td>-0.664147</td>\n",
              "      <td>-1.175568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-06-01 00:04:00</td>\n",
              "      <td>-1.075401</td>\n",
              "      <td>-0.988670</td>\n",
              "      <td>-1.050114</td>\n",
              "      <td>0.031327</td>\n",
              "      <td>-0.071754</td>\n",
              "      <td>-0.608493</td>\n",
              "      <td>-1.882582</td>\n",
              "      <td>0.161405</td>\n",
              "      <td>-1.650254</td>\n",
              "      <td>-0.726741</td>\n",
              "      <td>-0.817380</td>\n",
              "      <td>0.303134</td>\n",
              "      <td>1.740530</td>\n",
              "      <td>1.889123</td>\n",
              "      <td>1.950416</td>\n",
              "      <td>0.053002</td>\n",
              "      <td>-0.043201</td>\n",
              "      <td>1.477941</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc9609d6-7329-47b1-8598-0044ab27d240')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc9609d6-7329-47b1-8598-0044ab27d240 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc9609d6-7329-47b1-8598-0044ab27d240');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_weekdays_norm.head()"
      ],
      "metadata": {
        "id": "mQo9ewweclhz",
        "outputId": "6042b690-f3e8-4e1e-9216-9635eaa9a5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    time      PLN1      PLN2      PLN3      ULL1      ULL2  \\\n",
              "240  2020-06-01 04:00:00 -3.844526 -2.815111 -3.811858  1.679619  1.570822   \n",
              "241  2020-06-01 04:01:00 -3.846186 -3.787824 -3.823188  1.763631  1.696076   \n",
              "242  2020-06-01 04:02:00 -3.839272 -1.875102 -2.712874  1.852445  1.730759   \n",
              "243  2020-06-01 04:03:00 -3.842709 -3.088604 -3.827000  1.832063  1.744944   \n",
              "244  2020-06-01 04:04:00 -3.844287 -2.842539 -3.450520  1.753998  1.623568   \n",
              "\n",
              "         ULL3  COS_PHI1  COS_PHI2   COS_PHI3      FREQ     RC_DC     RC_AC  \\\n",
              "240  1.782563 -1.458455 -0.043591 -11.695581 -0.570289 -0.884008 -3.224201   \n",
              "241  1.843617 -1.467086 -2.835547 -11.782866  0.903443  2.133621 -3.224201   \n",
              "242  1.917486 -1.557711  0.058113  -1.543490  0.445873  0.624807 -1.273229   \n",
              "243  1.905749 -1.475716 -0.716154 -12.237347 -0.219683  0.624807 -1.923553   \n",
              "244  1.808403 -1.527502 -0.430725  -5.973931 -0.611886 -0.884008 -1.842262   \n",
              "\n",
              "      RC_50Hz  RC_150Hz  RC_<100Hz  RC_100Hz-1kHz  RC_>1kHz  RC_>10kHz  \n",
              "240 -1.568103 -1.701045  -1.466370      -3.271799 -2.865462  -1.695805  \n",
              "241 -1.568103 -1.701045  -1.466370      -3.357651 -2.939190  -1.695805  \n",
              "242 -0.765503 -1.118658  -0.885575      -1.211362 -0.948518  -0.928865  \n",
              "243 -1.568103 -1.312787  -1.272772      -2.069878 -1.538347  -0.928865  \n",
              "244 -0.765503 -1.312787  -1.272772      -2.069878 -1.464618  -0.928865  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7dcf18a-16b8-4e17-a1ed-454b4cf671e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>PLN1</th>\n",
              "      <th>PLN2</th>\n",
              "      <th>PLN3</th>\n",
              "      <th>ULL1</th>\n",
              "      <th>ULL2</th>\n",
              "      <th>ULL3</th>\n",
              "      <th>COS_PHI1</th>\n",
              "      <th>COS_PHI2</th>\n",
              "      <th>COS_PHI3</th>\n",
              "      <th>FREQ</th>\n",
              "      <th>RC_DC</th>\n",
              "      <th>RC_AC</th>\n",
              "      <th>RC_50Hz</th>\n",
              "      <th>RC_150Hz</th>\n",
              "      <th>RC_&lt;100Hz</th>\n",
              "      <th>RC_100Hz-1kHz</th>\n",
              "      <th>RC_&gt;1kHz</th>\n",
              "      <th>RC_&gt;10kHz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>2020-06-01 04:00:00</td>\n",
              "      <td>-3.844526</td>\n",
              "      <td>-2.815111</td>\n",
              "      <td>-3.811858</td>\n",
              "      <td>1.679619</td>\n",
              "      <td>1.570822</td>\n",
              "      <td>1.782563</td>\n",
              "      <td>-1.458455</td>\n",
              "      <td>-0.043591</td>\n",
              "      <td>-11.695581</td>\n",
              "      <td>-0.570289</td>\n",
              "      <td>-0.884008</td>\n",
              "      <td>-3.224201</td>\n",
              "      <td>-1.568103</td>\n",
              "      <td>-1.701045</td>\n",
              "      <td>-1.466370</td>\n",
              "      <td>-3.271799</td>\n",
              "      <td>-2.865462</td>\n",
              "      <td>-1.695805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>2020-06-01 04:01:00</td>\n",
              "      <td>-3.846186</td>\n",
              "      <td>-3.787824</td>\n",
              "      <td>-3.823188</td>\n",
              "      <td>1.763631</td>\n",
              "      <td>1.696076</td>\n",
              "      <td>1.843617</td>\n",
              "      <td>-1.467086</td>\n",
              "      <td>-2.835547</td>\n",
              "      <td>-11.782866</td>\n",
              "      <td>0.903443</td>\n",
              "      <td>2.133621</td>\n",
              "      <td>-3.224201</td>\n",
              "      <td>-1.568103</td>\n",
              "      <td>-1.701045</td>\n",
              "      <td>-1.466370</td>\n",
              "      <td>-3.357651</td>\n",
              "      <td>-2.939190</td>\n",
              "      <td>-1.695805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>2020-06-01 04:02:00</td>\n",
              "      <td>-3.839272</td>\n",
              "      <td>-1.875102</td>\n",
              "      <td>-2.712874</td>\n",
              "      <td>1.852445</td>\n",
              "      <td>1.730759</td>\n",
              "      <td>1.917486</td>\n",
              "      <td>-1.557711</td>\n",
              "      <td>0.058113</td>\n",
              "      <td>-1.543490</td>\n",
              "      <td>0.445873</td>\n",
              "      <td>0.624807</td>\n",
              "      <td>-1.273229</td>\n",
              "      <td>-0.765503</td>\n",
              "      <td>-1.118658</td>\n",
              "      <td>-0.885575</td>\n",
              "      <td>-1.211362</td>\n",
              "      <td>-0.948518</td>\n",
              "      <td>-0.928865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>2020-06-01 04:03:00</td>\n",
              "      <td>-3.842709</td>\n",
              "      <td>-3.088604</td>\n",
              "      <td>-3.827000</td>\n",
              "      <td>1.832063</td>\n",
              "      <td>1.744944</td>\n",
              "      <td>1.905749</td>\n",
              "      <td>-1.475716</td>\n",
              "      <td>-0.716154</td>\n",
              "      <td>-12.237347</td>\n",
              "      <td>-0.219683</td>\n",
              "      <td>0.624807</td>\n",
              "      <td>-1.923553</td>\n",
              "      <td>-1.568103</td>\n",
              "      <td>-1.312787</td>\n",
              "      <td>-1.272772</td>\n",
              "      <td>-2.069878</td>\n",
              "      <td>-1.538347</td>\n",
              "      <td>-0.928865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>2020-06-01 04:04:00</td>\n",
              "      <td>-3.844287</td>\n",
              "      <td>-2.842539</td>\n",
              "      <td>-3.450520</td>\n",
              "      <td>1.753998</td>\n",
              "      <td>1.623568</td>\n",
              "      <td>1.808403</td>\n",
              "      <td>-1.527502</td>\n",
              "      <td>-0.430725</td>\n",
              "      <td>-5.973931</td>\n",
              "      <td>-0.611886</td>\n",
              "      <td>-0.884008</td>\n",
              "      <td>-1.842262</td>\n",
              "      <td>-0.765503</td>\n",
              "      <td>-1.312787</td>\n",
              "      <td>-1.272772</td>\n",
              "      <td>-2.069878</td>\n",
              "      <td>-1.464618</td>\n",
              "      <td>-0.928865</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7dcf18a-16b8-4e17-a1ed-454b4cf671e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7dcf18a-16b8-4e17-a1ed-454b4cf671e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7dcf18a-16b8-4e17-a1ed-454b4cf671e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_weekend_norm.head()"
      ],
      "metadata": {
        "id": "TBgx07hRcodl",
        "outputId": "3834ab0d-844c-438e-9db0-e6bde69dbf32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  time      PLN1      PLN2      PLN3      ULL1      ULL2  \\\n",
              "0  2020-06-01 00:00:00 -0.520051 -0.469417 -0.491179 -0.852017 -1.003068   \n",
              "1  2020-06-01 00:01:00 -0.518390 -0.469592 -0.485656 -0.905465 -1.016009   \n",
              "2  2020-06-01 00:02:00 -0.520102 -0.348132 -0.489844 -0.987181 -1.028018   \n",
              "3  2020-06-01 00:03:00 -0.515582 -0.349161 -0.488267 -0.814230 -0.926227   \n",
              "4  2020-06-01 00:04:00 -0.519607 -0.348889 -0.488696 -0.899130 -0.965723   \n",
              "\n",
              "       ULL3  COS_PHI1  COS_PHI2  COS_PHI3      FREQ     RC_DC     RC_AC  \\\n",
              "0 -1.783292 -1.338808 -1.189834 -0.885658 -0.479759 -0.761410  1.276387   \n",
              "1 -1.803094 -1.356629 -1.159350 -0.870606 -1.233069  0.728477 -0.330467   \n",
              "2 -1.808977 -1.303165  0.693881 -0.921471 -1.217799 -0.761410 -0.330467   \n",
              "3 -1.693993 -1.082826  0.575856 -0.936003 -0.871684  0.728477 -0.211441   \n",
              "4 -1.768380 -1.355009  0.707168 -1.053303 -0.841144 -0.761410  0.919308   \n",
              "\n",
              "    RC_50Hz  RC_150Hz  RC_<100Hz  RC_100Hz-1kHz  RC_>1kHz  RC_>10kHz  \n",
              "0  1.388355  1.509262   1.555410       1.427389  1.381491   2.307679  \n",
              "1 -0.570467 -0.350376  -0.254028      -0.283821 -0.298828  -0.881879  \n",
              "2 -0.570467 -0.350376  -0.254028      -0.283821 -0.298828  -0.881879  \n",
              "3  0.082473 -0.195407  -0.254028      -0.218005 -0.174360  -0.881879  \n",
              "4  2.041296  2.439081   2.377882       0.703416  0.510214   1.669767  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9755e4ab-9876-465e-9f90-eb8991e43afd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>PLN1</th>\n",
              "      <th>PLN2</th>\n",
              "      <th>PLN3</th>\n",
              "      <th>ULL1</th>\n",
              "      <th>ULL2</th>\n",
              "      <th>ULL3</th>\n",
              "      <th>COS_PHI1</th>\n",
              "      <th>COS_PHI2</th>\n",
              "      <th>COS_PHI3</th>\n",
              "      <th>FREQ</th>\n",
              "      <th>RC_DC</th>\n",
              "      <th>RC_AC</th>\n",
              "      <th>RC_50Hz</th>\n",
              "      <th>RC_150Hz</th>\n",
              "      <th>RC_&lt;100Hz</th>\n",
              "      <th>RC_100Hz-1kHz</th>\n",
              "      <th>RC_&gt;1kHz</th>\n",
              "      <th>RC_&gt;10kHz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-06-01 00:00:00</td>\n",
              "      <td>-0.520051</td>\n",
              "      <td>-0.469417</td>\n",
              "      <td>-0.491179</td>\n",
              "      <td>-0.852017</td>\n",
              "      <td>-1.003068</td>\n",
              "      <td>-1.783292</td>\n",
              "      <td>-1.338808</td>\n",
              "      <td>-1.189834</td>\n",
              "      <td>-0.885658</td>\n",
              "      <td>-0.479759</td>\n",
              "      <td>-0.761410</td>\n",
              "      <td>1.276387</td>\n",
              "      <td>1.388355</td>\n",
              "      <td>1.509262</td>\n",
              "      <td>1.555410</td>\n",
              "      <td>1.427389</td>\n",
              "      <td>1.381491</td>\n",
              "      <td>2.307679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-06-01 00:01:00</td>\n",
              "      <td>-0.518390</td>\n",
              "      <td>-0.469592</td>\n",
              "      <td>-0.485656</td>\n",
              "      <td>-0.905465</td>\n",
              "      <td>-1.016009</td>\n",
              "      <td>-1.803094</td>\n",
              "      <td>-1.356629</td>\n",
              "      <td>-1.159350</td>\n",
              "      <td>-0.870606</td>\n",
              "      <td>-1.233069</td>\n",
              "      <td>0.728477</td>\n",
              "      <td>-0.330467</td>\n",
              "      <td>-0.570467</td>\n",
              "      <td>-0.350376</td>\n",
              "      <td>-0.254028</td>\n",
              "      <td>-0.283821</td>\n",
              "      <td>-0.298828</td>\n",
              "      <td>-0.881879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-06-01 00:02:00</td>\n",
              "      <td>-0.520102</td>\n",
              "      <td>-0.348132</td>\n",
              "      <td>-0.489844</td>\n",
              "      <td>-0.987181</td>\n",
              "      <td>-1.028018</td>\n",
              "      <td>-1.808977</td>\n",
              "      <td>-1.303165</td>\n",
              "      <td>0.693881</td>\n",
              "      <td>-0.921471</td>\n",
              "      <td>-1.217799</td>\n",
              "      <td>-0.761410</td>\n",
              "      <td>-0.330467</td>\n",
              "      <td>-0.570467</td>\n",
              "      <td>-0.350376</td>\n",
              "      <td>-0.254028</td>\n",
              "      <td>-0.283821</td>\n",
              "      <td>-0.298828</td>\n",
              "      <td>-0.881879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-06-01 00:03:00</td>\n",
              "      <td>-0.515582</td>\n",
              "      <td>-0.349161</td>\n",
              "      <td>-0.488267</td>\n",
              "      <td>-0.814230</td>\n",
              "      <td>-0.926227</td>\n",
              "      <td>-1.693993</td>\n",
              "      <td>-1.082826</td>\n",
              "      <td>0.575856</td>\n",
              "      <td>-0.936003</td>\n",
              "      <td>-0.871684</td>\n",
              "      <td>0.728477</td>\n",
              "      <td>-0.211441</td>\n",
              "      <td>0.082473</td>\n",
              "      <td>-0.195407</td>\n",
              "      <td>-0.254028</td>\n",
              "      <td>-0.218005</td>\n",
              "      <td>-0.174360</td>\n",
              "      <td>-0.881879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-06-01 00:04:00</td>\n",
              "      <td>-0.519607</td>\n",
              "      <td>-0.348889</td>\n",
              "      <td>-0.488696</td>\n",
              "      <td>-0.899130</td>\n",
              "      <td>-0.965723</td>\n",
              "      <td>-1.768380</td>\n",
              "      <td>-1.355009</td>\n",
              "      <td>0.707168</td>\n",
              "      <td>-1.053303</td>\n",
              "      <td>-0.841144</td>\n",
              "      <td>-0.761410</td>\n",
              "      <td>0.919308</td>\n",
              "      <td>2.041296</td>\n",
              "      <td>2.439081</td>\n",
              "      <td>2.377882</td>\n",
              "      <td>0.703416</td>\n",
              "      <td>0.510214</td>\n",
              "      <td>1.669767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9755e4ab-9876-465e-9f90-eb8991e43afd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9755e4ab-9876-465e-9f90-eb8991e43afd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9755e4ab-9876-465e-9f90-eb8991e43afd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this moment we have six different datasets to use:\n",
        "* dataset_clean (Whole dataset)\n",
        "* dataset_weekdays (Entries from weekdays from 4:00 to 19:30)\n",
        "* dataset_weekend (Entries from Weekends and from weekdays from 19:30 to 4:00)\n",
        "* dataset_norm (Whole dataset, mean/stddev normalised)\n",
        "* dataset_weekdays_norm (Entries from weekdays from 4:00 to 19:30, mean/stddev normalised)\n",
        "* dataset_weekend_norm (Entries from Weekends and from weekdays from 19:30 to 4:00, mean/stddev normalised)"
      ],
      "metadata": {
        "id": "hnu9AcwDW8ZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Covariance matrix of all features"
      ],
      "metadata": {
        "id": "AqX61PrGpaxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "set_ = dataset_norm.iloc[:,1:].values\n",
        "\n",
        "print(set_.shape)\n",
        "print(type(set_[0][0]))\n",
        "\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "cov_matrix = np.cov(set_.T)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "# Adds subplot on position 1\n",
        "ax = fig.add_subplot(121)\n",
        "ax.matshow(cov_matrix)\n",
        "plt.show()\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(eigenvalues)\n",
        "\n",
        "explained_variance = []\n",
        "\n",
        "for i in eigenvalues:\n",
        "    explained_variance.append(i/sum(eigenvalues))\n",
        "\n",
        "print(explained_variance)"
      ],
      "metadata": {
        "id": "wRWPIDDnpWhW",
        "outputId": "b51ab07e-891a-400e-efd6-0d1aed541f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(63360, 18)\n",
            "<class 'numpy.float64'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.572 2.940 1.352 0.989 0.970 0.891 0.405 0.251 0.230 0.177 0.072 0.052\n",
            " 0.039 0.031 0.012 0.004 0.004 0.007]\n",
            "[0.5317647804810274, 0.16335739298653476, 0.07511546472382995, 0.054921627068028424, 0.05390616867076577, 0.04952232661739343, 0.022486349463995598, 0.013932731902136385, 0.012792662672300325, 0.009829012007199104, 0.004024926426955747, 0.002893959610103366, 0.002163271201445878, 0.001741116222641519, 0.0006928432299862775, 0.00021826337426103455, 0.00023351820917083064, 0.0004035851322244031]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJeKUzS0jDhq"
      },
      "source": [
        "# Preparation Training and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvocvIBA292-"
      },
      "source": [
        "Once the dataset is prepared, make batches of data,put them togheter in an array and split them into train and test sets.\n",
        "After looking through the dataset and the features, i decided to takeonly the values with a timestap of a weekday between 4:00 and 19:30. In many of the features in the interval outside those timestamps there i only noise, which can be a sign that the machine is off in that time interval."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(dataset_norm):\n",
        "\n",
        "  # Create a dataset with pairs data / Target (in this case data is one measure (18 features) and target is the next measure (18 features))\n",
        "  # When you plug in one measure, the model should out the next measure\n",
        "\n",
        "  pair_set = []\n",
        "\n",
        "  for i in tqdm(range(len(dataset_norm) -1)):\n",
        "    data = np.array([j for j in dataset_norm.iloc[i, 1:]])\n",
        "    target = np.array([j for j in dataset_norm.iloc[i+1, 1:]])\n",
        "    \n",
        "    pair_set.append((data, target))\n",
        "\n",
        "  dataset_pairs = np.array(pair_set)\n",
        "\n",
        "  training_data_pairs, testing_data_pairs = train_test_split(dataset_pairs, test_size=0.1, random_state=25)\n",
        "\n",
        "  data = []\n",
        "  target = []\n",
        "  for i in training_data_pairs:\n",
        "    data.append(i[0])\n",
        "    target.append(i[1])\n",
        "\n",
        "  training_data = torch.from_numpy(np.array(data)).float().to(device)\n",
        "  training_target = torch.from_numpy(np.array(target)).float().to(device)\n",
        "\n",
        "  data = []\n",
        "  target = []\n",
        "  for i in testing_data_pairs:\n",
        "    data.append(i[0])\n",
        "    target.append(i[1])\n",
        "\n",
        "  test_data = torch.from_numpy(np.array(data)).float().to(device)\n",
        "  test_target = torch.from_numpy(np.array(target)).float().to(device)\n",
        "\n",
        "  print(f'length of training set (whole dataset): {training_data.shape[0]}')\n",
        "  print(f'length of test set (whole dataset): {test_data.shape[0]}')\n",
        "  print('\\n')\n",
        "\n",
        "  # Create data loader to feed the FFN in mini batches\n",
        "\n",
        "  loader_train = torch.utils.data.DataLoader(\n",
        "      dataset=torch.utils.data.TensorDataset(training_data, training_target),\n",
        "      batch_size=64,\n",
        "      shuffle=True\n",
        "  )\n",
        "\n",
        "  # Create data loader for testing the model\n",
        "  loader_test = torch.utils.data.DataLoader(\n",
        "      dataset=torch.utils.data.TensorDataset(test_data, test_target),\n",
        "      batch_size=64,\n",
        "      shuffle=True\n",
        "  )\n",
        "\n",
        "  return loader_train, loader_test"
      ],
      "metadata": {
        "id": "eI6P8KvabrMO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader_train, loader_test = create_dataloaders(dataset_norm)"
      ],
      "metadata": {
        "id": "kgHc7L9_cfN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3364b67e-be3c-412e-c5f1-65b54dcb821c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63359/63359 [00:33<00:00, 1883.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of training set (whole dataset): 57023\n",
            "length of test set (whole dataset): 6336\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Baseline Model\n",
        "\n",
        "I am taking the Last step as prediction of all features to create a baselinemodel. I will use this baseline model to compare the results of the actual model with it. Everything that works better than this baseline model could be an improvement."
      ],
      "metadata": {
        "id": "VazanvM-f9cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "losses_train = []\n",
        "\n",
        "for i in loader_train:\n",
        "  output = i[0]\n",
        "  target = i[1]\n",
        "  loss = criterion(output, target)\n",
        "  losses_train.append(loss.item())\n",
        "\n",
        "losses_test = []\n",
        "\n",
        "for i in loader_test:\n",
        "  output = i[0]\n",
        "  target = i[1]\n",
        "  loss = criterion(output, target)\n",
        "  losses_test.append(loss.item())\n",
        "\n",
        "print(\"Training set\")\n",
        "print(\"Mean Loss of baselinemodel: \", np.mean(losses_train))\n",
        "print(\"Standard deviation Loss of baselinemodel: \", np.std(losses_train))\n",
        "print('\\n')\n",
        "print(\"Test set\")\n",
        "print(\"Mean Loss of baselinemodel: \", np.mean(losses_test))\n",
        "print(\"Standard deviation Loss of baselinemodel: \", np.std(losses_test))\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me_JXHtgZyuE",
        "outputId": "99f9cc55-0963-4513-dfb3-e8a1442543e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set\n",
            "Mean Loss of baselinemodel:  0.4737866011131492\n",
            "Standard deviation Loss of baselinemodel:  0.09320007350491984\n",
            "\n",
            "\n",
            "Test set\n",
            "Mean Loss of baselinemodel:  0.4659305717607941\n",
            "Standard deviation Loss of baselinemodel:  0.08776944196887589\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a simple Feed Forward Neural Network\n",
        "\n",
        "I trained a simple FFN Network to have a second baseline model. The final model training should have also a better performance than this FFN."
      ],
      "metadata": {
        "id": "aX-B-7HMqFeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN_relu(nn.Module):\n",
        "\n",
        "    def __init__(self, D_in, D_out):\n",
        "        super(ANN_relu, self).__init__()\n",
        "        self.linear1 = nn.Linear(D_in, 180)\n",
        "        self.linear2 = nn.Linear(180, 360)\n",
        "        self.linear3 = nn.Linear(360, 360)\n",
        "        self.linear4 = nn.Linear(360, 180)\n",
        "        self.linear5 = nn.Linear(180, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        x = torch.relu(self.linear3(x))\n",
        "        x = torch.relu(self.linear4(x))\n",
        "        return self.linear5(x)\n",
        "\n",
        "# This function trains the model for one epoch\n",
        "def train(model, criterion, optimizer, train_loader, test_loader, n_epochs):\n",
        "\n",
        "    epoch_loss_train = []\n",
        "    epoch_loss_test = []\n",
        "\n",
        "    for e in range(1, n_epochs +1):\n",
        "      print(f'\\nEpoch {e}:')\n",
        "\n",
        "      print('Train')\n",
        "      model.train()\n",
        "\n",
        "      for i in tqdm(train_loader):\n",
        "\n",
        "        data, target = i[0], i[1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward Pass\n",
        "        output = model(data)\n",
        "\n",
        "        #Compute loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        #Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        #Optimization\n",
        "        optimizer.step()\n",
        "\n",
        "      losses = []\n",
        "\n",
        "      print('\\nTest with training set')\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i in tqdm(train_loader):\n",
        "\n",
        "          data, target = i[0], i[1]\n",
        "\n",
        "          output = model(data)\n",
        "              \n",
        "          losses.append (float(criterion(output, target).item()))\n",
        "\n",
        "      print('\\nCurrent Mean loss Train: ', np.mean(losses))\n",
        "      epoch_loss_train.append(losses)\n",
        "\n",
        "      losses = []\n",
        "\n",
        "      print('\\nTest with test set')\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i in tqdm(test_loader):\n",
        "\n",
        "          data, target = i[0], i[1]\n",
        "\n",
        "          output = model(data)\n",
        "            \n",
        "          losses.append (float(criterion(output, target).item()))\n",
        "\n",
        "\n",
        "      print('\\nCurrent Mean loss: ', np.mean(losses))\n",
        "      epoch_loss_test.append(losses)\n",
        "\n",
        "    return model, epoch_loss_train, epoch_loss_test"
      ],
      "metadata": {
        "id": "n9961Y_qY190"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "lr=0.01\n",
        "\n",
        "# Create model FFN instance\n",
        "model_FFN_whole = ANN_relu(18, 18).to(device)\n",
        "print(model_FFN_whole)\n",
        "\n",
        "# Define Loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer_whole = torch.optim.SGD(model_FFN_whole.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "train_FFN = False\n",
        "\n",
        "params_not_trained_whole = model_FFN_whole.parameters()\n",
        "\n",
        "if train_FFN is True:\n",
        "  trained_model_FFN_whole , train_losses_whole, test_losses_whole = train(model_FFN_whole, criterion, optimizer_whole, loader_train, loader_test, n_epochs)\n"
      ],
      "metadata": {
        "id": "XXhL658rVs8_",
        "outputId": "6ebe466a-ad5b-4b88-99f9-b148e97aed57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN_relu(\n",
            "  (linear1): Linear(in_features=18, out_features=180, bias=True)\n",
            "  (linear2): Linear(in_features=180, out_features=360, bias=True)\n",
            "  (linear3): Linear(in_features=360, out_features=360, bias=True)\n",
            "  (linear4): Linear(in_features=360, out_features=180, bias=True)\n",
            "  (linear5): Linear(in_features=180, out_features=18, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_FFN is True:\n",
        "\n",
        "  # Show results of the loss function whole\n",
        "\n",
        "  fig = plt.figure(figsize = (10,10))\n",
        "\n",
        "  ax = fig.add_subplot(111)\n",
        "  plt.ion()\n",
        "\n",
        "  fig.show()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  baseline = [np.mean(losses_train) for i in range(len(train_losses_whole))]\n",
        "\n",
        "  ax.plot(baseline, label='Baseline')\n",
        "  ax.plot([np.mean(i) for i in train_losses_whole], label= 'Train_loss')\n",
        "  ax.plot([np.mean(i) for i in test_losses_whole], label= 'Test_loss')\n",
        "  ax.set_title(\"Full Forward Neural Network (Whole dataset)\")\n",
        "  ax.set_xlabel('Epoch')\n",
        "  ax.set_ylabel('Mean Squared Error')\n",
        "  ax.legend()\n",
        "  fig.canvas.draw()"
      ],
      "metadata": {
        "id": "7kSeRG2ILQ65"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We do the same process with the Weekend dataset and the Weekdays dataset, to see if its any improve taking only those datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "HbE_EzsRLT0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader_train_weekdays, loader_test_weekdays = create_dataloaders(dataset_weekdays_norm)\n",
        "loader_train_weekend, loader_test_weekend = create_dataloaders(dataset_weekend_norm)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "losses_train_weekdays = []\n",
        "\n",
        "for i in loader_train_weekdays:\n",
        "  output = i[0]\n",
        "  target = i[1]\n",
        "  loss = criterion(output, target)\n",
        "  losses_train_weekdays.append(loss.item())\n",
        "\n",
        "losses_test_weekend = []\n",
        "\n",
        "for i in loader_test_weekend:\n",
        "  output = i[0]\n",
        "  target = i[1]\n",
        "  loss = criterion(output, target)\n",
        "  losses_test_weekend.append(loss.item())\n",
        "\n",
        "print(\"Training set\")\n",
        "print(\"Mean Loss of baselinemodel Weekdays: \", np.mean(losses_train_weekdays))\n",
        "print(\"Standard deviation Loss of baselinemodel Weekdays: \", np.std(losses_train_weekdays))\n",
        "print('\\n')\n",
        "print(\"Test set\")\n",
        "print(\"Mean Loss of baselinemodel Weekend: \", np.mean(losses_test_weekend))\n",
        "print(\"Standard deviation Loss of baselinemodel Weekend: \", np.std(losses_test_weekend))\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "YIq_bRl3GkCY",
        "outputId": "a28bae34-3929-4501-918c-1adfd0b0fe93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29791/29791 [00:15<00:00, 1872.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of training set (whole dataset): 26811\n",
            "length of test set (whole dataset): 2980\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33567/33567 [00:17<00:00, 1908.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of training set (whole dataset): 30210\n",
            "length of test set (whole dataset): 3357\n",
            "\n",
            "\n",
            "Training set\n",
            "Mean Loss of baselinemodel Weekdays:  0.42486975091311696\n",
            "Standard deviation Loss of baselinemodel Weekdays:  0.11349020780204344\n",
            "\n",
            "\n",
            "Test set\n",
            "Mean Loss of baselinemodel Weekend:  0.7605142109798935\n",
            "Standard deviation Loss of baselinemodel Weekend:  0.11978187220861101\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "lr=0.01\n",
        "\n",
        "# Create model FFN instance\n",
        "model_FFN_weekday = ANN_relu(18, 18).to(device)\n",
        "print(model_FFN_weekday)\n",
        "\n",
        "# Define Loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer_weekday = torch.optim.SGD(model_FFN_weekday.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "train_FFN = False\n",
        "\n",
        "params_not_trained_weekday = model_FFN_weekday.parameters()\n",
        "\n",
        "if train_FFN is True:\n",
        "  trained_model_FFN_weekday , train_losses_weekday, test_losses_weekday = train(model_FFN_weekday, criterion, optimizer_weekday, loader_train_weekdays, loader_test_weekdays, n_epochs)\n",
        "\n",
        "\n",
        "n_epochs = 50\n",
        "lr=0.01\n",
        "\n",
        "# Create model FFN instance\n",
        "model_FFN_weekend = ANN_relu(18, 18).to(device)\n",
        "print(model_FFN_weekend)\n",
        "\n",
        "# Define Loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer_weekend = torch.optim.SGD(model_FFN_weekend.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "train_FFN = False\n",
        "\n",
        "params_not_trained_weekend = model_FFN_weekend.parameters()\n",
        "\n",
        "if train_FFN is True:\n",
        "  trained_model_FFN_weekend , train_losses_weekend, test_losses_weekend = train(model_FFN_weekend, criterion, optimizer_weekend, loader_train_weekend, loader_test_weekend, n_epochs)"
      ],
      "metadata": {
        "id": "ssGQuSMwGwsX",
        "outputId": "456c2066-1a47-4fcf-abfa-d33bcea75b87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN_relu(\n",
            "  (linear1): Linear(in_features=18, out_features=180, bias=True)\n",
            "  (linear2): Linear(in_features=180, out_features=360, bias=True)\n",
            "  (linear3): Linear(in_features=360, out_features=360, bias=True)\n",
            "  (linear4): Linear(in_features=360, out_features=180, bias=True)\n",
            "  (linear5): Linear(in_features=180, out_features=18, bias=True)\n",
            ")\n",
            "ANN_relu(\n",
            "  (linear1): Linear(in_features=18, out_features=180, bias=True)\n",
            "  (linear2): Linear(in_features=180, out_features=360, bias=True)\n",
            "  (linear3): Linear(in_features=360, out_features=360, bias=True)\n",
            "  (linear4): Linear(in_features=360, out_features=180, bias=True)\n",
            "  (linear5): Linear(in_features=180, out_features=18, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_FFN is True:\n",
        "\n",
        "  # Show results of the loss function weekdays\n",
        "\n",
        "  fig = plt.figure(figsize = (10,10))\n",
        "\n",
        "  ax = fig.add_subplot(111)\n",
        "  plt.ion()\n",
        "\n",
        "  fig.show()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  baseline = [np.mean(losses_train_weekdays) for i in range(len(train_losses_weekday))]\n",
        "\n",
        "  ax.plot(baseline, label='Baseline')\n",
        "  ax.plot([np.mean(i) for i in train_losses_weekday], label= 'Train_loss')\n",
        "  ax.plot([np.mean(i) for i in test_losses_weekday], label= 'Test_loss')\n",
        "  ax.set_title(\"Full Forward Neural Network (Weekday dataset)\")\n",
        "  ax.set_xlabel('Epoch')\n",
        "  ax.set_ylabel('Mean Squared Error')\n",
        "  ax.legend()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  # Show results of the loss function weekends\n",
        "\n",
        "  fig = plt.figure(figsize = (10,10))\n",
        "\n",
        "  ax = fig.add_subplot(111)\n",
        "  plt.ion()\n",
        "\n",
        "  fig.show()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  baseline = [np.mean(losses_train_weekend) for i in range(len(train_losses_weekend))]\n",
        "\n",
        "  ax.plot(baseline, label='Baseline')\n",
        "  ax.plot([np.mean(i) for i in train_losses_weekend], label= 'Train_loss')\n",
        "  ax.plot([np.mean(i) for i in test_losses_weekend], label= 'Test_loss')\n",
        "  ax.set_title(\"Full Forward Neural Network (Weekend dataset)\")\n",
        "  ax.set_xlabel('Epoch')\n",
        "  ax.set_ylabel('Mean Squared Error')\n",
        "  ax.legend()\n",
        "  fig.canvas.draw()"
      ],
      "metadata": {
        "id": "Lw2ntQMUafOF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions FFN\n",
        "\n",
        "We can see that using the weekends or weekdays dataset doesn't bring any improvement. Therefore we can forget them and use only the whole dataset for the next models."
      ],
      "metadata": {
        "id": "nzJPbmTPOleH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Model\n",
        "\n",
        "We train before a standard RNN and see which results we can expected with a small and easy solution.\n",
        "I am using this template (https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb) and make changes using it as a base.\n",
        "\n",
        "1) torch.nn.RNN (https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
        "\n",
        "Parameters\n",
        "* input_size – The number of expected features in the input x\n",
        "* hidden_size – The number of features in the hidden state h\n",
        "* num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
        "* nonlinearity – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
        "* bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "* batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
        "* dropout – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
        "* bidirectional – If True, becomes a bidirectional RNN. Default: False\n",
        "\n",
        "2) torch.nn.Linear (https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "\n",
        "Parameters\n",
        "* in_features – size of each input sample\n",
        "* out_features – size of each output sample\n",
        "* bias – If set to False, the layer will not learn an additive bias. Default: True\n"
      ],
      "metadata": {
        "id": "wLCXaTbSsHRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers, device, batch_first = True, dropout = 0):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "\n",
        "        #Defining the layers\n",
        "        self.fc1 = nn.Linear(input_size, input_size)\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size = hidden_dim, num_layers = n_layers, batch_first = batch_first, nonlinearity='relu', dropout = dropout)   \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size).to(self.device)\n",
        "\n",
        "        out = torch.relu(self.fc1(x))\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(out, hidden)\n",
        "\n",
        "        out = torch.relu(out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "VOL1v_mjsEDe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_RNN(model, criterion, optimizer, train_loader, test_loader, n_epochs):\n",
        "\n",
        "  epoch_loss_train = []\n",
        "  epoch_loss_test = []\n",
        "\n",
        "  # Training Run\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "    print(f'Epoch {epoch}')\n",
        "\n",
        "    losses_train = []\n",
        "    losses_test = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    print('\\nTraining:')\n",
        "    for i in tqdm(train_loader):\n",
        "\n",
        "      input = i[0]\n",
        "\n",
        "      target = i[1]\n",
        "\n",
        "      output, hidden = model_rnn(input)\n",
        "\n",
        "      #Compute loss\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      #Backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      #Optimization\n",
        "      optimizer.step()\n",
        "\n",
        "    print('\\nTest with training set')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i in tqdm(train_loader):\n",
        "        input = i[0]\n",
        "        target = i[1]\n",
        "\n",
        "        output, hidden = model(input)\n",
        "\n",
        "        #Compute loss\n",
        "        losses_train.append (float(criterion(output, target).item()))\n",
        "\n",
        "    print('\\nTest with test set')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i in tqdm(test_loader):\n",
        "        input = i[0]\n",
        "        target = i[1]\n",
        "\n",
        "        output, hidden = model(input)\n",
        "\n",
        "        #Compute loss\n",
        "        losses_test.append (float(criterion(output, target).item()))\n",
        "\n",
        "\n",
        "    print('\\nCurrent Mean loss Train Set: ', np.mean(losses_train))\n",
        "    epoch_loss_train.append(np.mean(losses_train))\n",
        "\n",
        "    print('\\nCurrent Mean loss Test Set: ', np.mean(losses_test))\n",
        "    epoch_loss_test.append(np.mean(losses_test))\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "  return epoch_loss_train, epoch_loss_test, model"
      ],
      "metadata": {
        "id": "IzAT4IPPwfZx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model_rnn = RNN(input_size = 18,\n",
        "                output_size = 18,\n",
        "                hidden_dim = 64,\n",
        "                n_layers = 1,\n",
        "                batch_first = True,\n",
        "                dropout = 0,\n",
        "                device = device)\n",
        "\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model_rnn = model_rnn.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 50\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr)\n",
        "\n",
        "train_RNN = False\n",
        "\n",
        "if train_RNN is True:\n",
        "  train_losses, test_losses, trained_model_RNN = training_RNN(model_rnn, criterion, optimizer, loader_train, loader_test, n_epochs)"
      ],
      "metadata": {
        "id": "kZQzvqz0aU5r"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if train_RNN is True:\n",
        "\n",
        "  # Show results of the loss function\n",
        "\n",
        "  fig = plt.figure()\n",
        "\n",
        "  ax = fig.add_subplot(111)\n",
        "  plt.ion()\n",
        "\n",
        "  fig.show()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  baseline = [np.mean(losses_train) for i in range(len(train_losses))]\n",
        "\n",
        "  ax.plot(baseline)\n",
        "  ax.plot([np.mean(i) for i in train_losses])\n",
        "  ax.plot([np.mean(i) for i in test_losses])\n",
        "  ax.set_title(\"Mean Squared Error RNN\")\n",
        "  fig.canvas.draw()"
      ],
      "metadata": {
        "id": "989Pq-hNLX7r"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last basemodel i am going to use is a simple RNN. The final model should also have a better performance than this RNN."
      ],
      "metadata": {
        "id": "Y93gczvmqiT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequece_dataloaders(dataset_norm):\n",
        "\n",
        "  # Create a dataset with pairs data / Target (in this case data is one sequence of 30 measures (18 features) and target are the next sequence of 30 \n",
        "  # measures (18 features)). When you plug in one measure, the model should out the next measure\n",
        "\n",
        "  pair_set = []\n",
        "\n",
        "  for i in tqdm(range(len(dataset_norm) - 60)):\n",
        "    data = np.array(dataset_norm.iloc[i:i+30, 1:])\n",
        "    target = np.array(dataset_norm.iloc[i+30:i+60, 1:])\n",
        "    \n",
        "    pair_set.append((data, target))\n",
        "\n",
        "  dataset_pairs = np.array(pair_set)\n",
        "\n",
        "  training_data_pairs, testing_data_pairs = train_test_split(dataset_pairs, test_size=0.1, random_state=25)\n",
        "\n",
        "  data = []\n",
        "  target = []\n",
        "  for i in training_data_pairs:\n",
        "    data.append(i[0])\n",
        "    target.append(i[1])\n",
        "\n",
        "  training_data = torch.from_numpy(np.array(data)).float().to(device)\n",
        "  training_target = torch.from_numpy(np.array(target)).float().to(device)\n",
        "\n",
        "  data = []\n",
        "  target = []\n",
        "  for i in testing_data_pairs:\n",
        "    data.append(i[0])\n",
        "    target.append(i[1])\n",
        "\n",
        "  test_data = torch.from_numpy(np.array(data)).float().to(device)\n",
        "  test_target = torch.from_numpy(np.array(target)).float().to(device)\n",
        "\n",
        "  print(f'length of training set (whole dataset): {training_data.shape[0]}')\n",
        "  print(f'length of test set (whole dataset): {test_data.shape[0]}')\n",
        "  print('\\n')\n",
        "\n",
        "  # Create data loader to feed the FFN in mini batches\n",
        "\n",
        "  loader_train = torch.utils.data.DataLoader(\n",
        "      dataset=torch.utils.data.TensorDataset(training_data, training_target),\n",
        "      batch_size=15,\n",
        "      shuffle=False\n",
        "  )\n",
        "\n",
        "  # Create data loader for testing the model\n",
        "  loader_test = torch.utils.data.DataLoader(\n",
        "      dataset=torch.utils.data.TensorDataset(test_data, test_target),\n",
        "      batch_size=15,\n",
        "      shuffle=False\n",
        "  )\n",
        "\n",
        "  return loader_train, loader_test"
      ],
      "metadata": {
        "id": "6E3nOOEDkOCp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader_train, loader_test = create_sequece_dataloaders(dataset_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loolRFt_jhgV",
        "outputId": "5fc23f9a-3554-4764-ceb7-1aeece6673d2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63300/63300 [00:35<00:00, 1769.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of training set (whole dataset): 56970\n",
            "length of test set (whole dataset): 6330\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "losses_train = []\n",
        "\n",
        "for i in loader_train:\n",
        "  output = i[0]\n",
        "  target = i[1]\n",
        "  loss = criterion(output, target)\n",
        "  losses_train.append(loss.item())\n",
        "\n",
        "losses_test = []\n",
        "\n",
        "for i in loader_test:\n",
        "  output = i[0]\n",
        "  target = i[1]\n",
        "  loss = criterion(output, target)\n",
        "  losses_test.append(loss.item())\n",
        "\n",
        "print(\"Training set\")\n",
        "print(\"Mean Loss of baselinemodel: \", np.mean(losses_train))\n",
        "print(\"Standard deviation Loss of baselinemodel: \", np.std(losses_train))\n",
        "print('\\n')\n",
        "print(\"Test set\")\n",
        "print(\"Mean Loss of baselinemodel: \", np.mean(losses_test))\n",
        "print(\"Standard deviation Loss of baselinemodel: \", np.std(losses_test))\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeZYKAbqkvUY",
        "outputId": "2d8dbb93-63e1-4039-f3fe-a64706342135"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set\n",
            "Mean Loss of baselinemodel:  0.7443867036312615\n",
            "Standard deviation Loss of baselinemodel:  0.11228539154408188\n",
            "\n",
            "\n",
            "Test set\n",
            "Mean Loss of baselinemodel:  0.750549325445817\n",
            "Standard deviation Loss of baselinemodel:  0.10785673408274579\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXX8lu3jjDhr"
      },
      "source": [
        "# Transformer Model settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dUCsWWo292_"
      },
      "source": [
        "Now, we define a class with the transformer model that we are going to use:\n",
        "\n",
        "Using the already written pytorch library for Transformers:\n",
        "\n",
        "1) torch.nn.TransformerEncoderLayer (https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)\n",
        "\n",
        "*   d_model –> the number of expected features in the input (required).\n",
        "*   nhead –> the number of heads in the multiheadattention models (required).\n",
        "*   dropout –> the dropout value (default=0.1).\n",
        "*   activation –> the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. (default: relu)\n",
        "*   layer_norm_eps –> the eps value in layer normalization components (default=1e-5).\n",
        "*   batch_first –> If True, then the input and output tensors are provided as (batch, seq, feature). (default: False)\n",
        "*   norm_first –> if True, layer norm is done prior to attention and feedforward operations, respectivaly. Otherwise it’s done after. (default: False (after))\n",
        "\n",
        "2) torch.nn.TransformerDecoderLayer\n",
        "\n",
        "* d_model –> the number of expected features in the input (required).\n",
        "* nhead –> the number of heads in the multiheadattention models (required).\n",
        "* dim_feedforward –> the dimension of the feedforward network model (default=2048).\n",
        "* dropout –> the dropout value (default=0.1).\n",
        "* activation –> the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu\n",
        "* layer_norm_eps –> the eps value in layer normalization components (default=1e-5).\n",
        "* batch_first –> If True, then the input and output tensors are provided as (batch, seq, feature). Default: False.\n",
        "* norm_first –> if True, layer norm is done prior to self attention, multihead attention and feedforward operations, respectivaly. Otherwise it’s done after. Default: False (after).\n",
        "\n",
        "3) torch.nn.TransformerEncoder\n",
        "\n",
        "* encoder_layer –> an instance of the TransformerEncoderLayer() class (required).\n",
        "* num_layers –> the number of sub-encoder-layers in the encoder (required).\n",
        "* norm –> the layer normalization component (optional).\n",
        "\n",
        "\n",
        "4) torch.nn.TransformerDecoder\n",
        "\n",
        "* decoder_layer – an instance of the TransformerDecoderLayer() class (required).\n",
        "* num_layers – the number of sub-decoder-layers in the decoder (required).\n",
        "* norm – the layer normalization component (optional).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tCC_Bava293A",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_len: int, dim_model: int, device):\n",
        "    \n",
        "    # Tensor with the positions of every sequence element (0 to seq_len)\n",
        "    pos = torch.arange(seq_len, dtype=float32, device=device).reshape(1, -1, 1)\n",
        "    \n",
        "    # Tensor with the positions of every feature in the sequence (0 to dim_model)\n",
        "    dim = torch.arange(dim_model, dtype=float32, device=device).reshape(1, 1, -1)\n",
        "\n",
        "    phase = pos / (1e4 ** (torch.div(dim, dim_model, rounding_mode='floor')))\n",
        "\n",
        "    position_encoding = torch.where(dim.long() % 2 == 0, sin(phase), cos(phase))\n",
        "\n",
        "    return position_encoding.to(device)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, feature_size, output_size, num_encoder_layers, num_heads, num_decoder_layers, device, dropout: float =0.1, batch_first: bool = False):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model= feature_size, nhead= num_heads, dropout=dropout, device=device, batch_first=batch_first)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model= feature_size, nhead= num_heads, dropout=dropout, device=device, batch_first=batch_first)\n",
        "        \n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers= num_encoder_layers)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers= num_decoder_layers)\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.device = device\n",
        "\n",
        "    def generate_square_mask(self, dim):\n",
        "        return torch.triu(torch.ones(dim, dim) * float('-inf'), diagonal=1).to(self.device)\n",
        "        \n",
        "    def forward (self, src):\n",
        "        \n",
        "        mask = self.generate_square_mask(len(src))\n",
        "\n",
        "        #src_pos = src + positional_encoding(src.shape[1], src.shape[2], self.device)\n",
        "\n",
        "        output = self.encoder (src, mask)\n",
        "        \n",
        "        output = self.decoder (src, output, mask)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should define an optimizer too.\n",
        "For this, we use the pytorch library:\n",
        "\n",
        "* SGD –> Stochastic gradient descent.\n",
        "\n",
        "1) torch.optim.SDG (https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)\n",
        "\n",
        "* params (iterable) – iterable of parameters to optimize or dicts defining parameter groups\n",
        "* lr (float) – learning rate\n",
        "* momentum (float, optional) – momentum factor (default: 0)\n",
        "* weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\n",
        "* dampening (float, optional) – dampening for momentum (default: 0)\n",
        "* nesterov (bool, optional) – enables Nesterov momentum (default: False)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8EANo5UFE15A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_transformer(model, optimizer, criterion, train_loader, test_loader, n_epochs):\n",
        "  epoch_loss_train = []\n",
        "  epoch_loss_test = []\n",
        "\n",
        "  for e in range(1, n_epochs + 1):\n",
        "\n",
        "    print(f'Epoch: {e} of {n_epochs}')\n",
        "\n",
        "    print('Training:')\n",
        "    model.train()\n",
        "\n",
        "    for i in tqdm(train_loader):\n",
        "\n",
        "      # Initialize optimizer gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      input = i[0]\n",
        "\n",
        "      target = i[1]\n",
        "\n",
        "      net_out = model.forward(input)\n",
        "\n",
        "      #Compute loss\n",
        "      loss = criterion(net_out, target)\n",
        "\n",
        "      #Backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      #Optimization\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    print('\\nTest with training set')\n",
        "    losses_train = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i in tqdm(train_loader):\n",
        "        input = i[0]\n",
        "        target = i[1]\n",
        "\n",
        "        net_out = model.forward(input)\n",
        "\n",
        "        #Compute loss\n",
        "        losses_train.append (float(criterion(net_out, target).item()))\n",
        "\n",
        "    \n",
        "    print('\\nCurrent Mean loss Train Set: ', np.mean(losses_train))\n",
        "    epoch_loss_train.append(losses_train)\n",
        "\n",
        "    print('\\nTest with training set')\n",
        "    losses_test = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i in tqdm(test_loader):\n",
        "        input = i[0]\n",
        "        target = i[1]\n",
        "\n",
        "        net_out = model.forward(input)\n",
        "\n",
        "        #Compute loss\n",
        "        losses_test.append (float(criterion(net_out, target).item()))\n",
        "\n",
        "    print('\\nCurrent Mean loss Test Set: ', np.mean(losses_test))\n",
        "    epoch_loss_test.append(losses_test)\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "  return model, epoch_loss_train, epoch_loss_test"
      ],
      "metadata": {
        "id": "SZ8UZSHPQLT5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup model Ok\n",
            "Setup optimizer Ok\n",
            "Epoch: 1 of 50\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3798/3798 [01:42<00:00, 36.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test with training set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3798/3798 [00:24<00:00, 157.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Current Mean loss Train Set:  1.0027310190513423\n",
            "\n",
            "Test with training set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [00:02<00:00, 156.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Current Mean loss Test Set:  1.0083856126425956\n",
            "\n",
            "\n",
            "Epoch: 2 of 50\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3798/3798 [01:42<00:00, 36.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test with training set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3798/3798 [00:23<00:00, 159.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Current Mean loss Train Set:  1.002175686216907\n",
            "\n",
            "Test with training set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [00:02<00:00, 159.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Current Mean loss Test Set:  1.007734998283793\n",
            "\n",
            "\n",
            "Epoch: 3 of 50\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 257/3798 [00:06<01:35, 37.04it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-26ed60183b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_transformer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mtrained_model_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-efe2077d6e12>\u001b[0m in \u001b[0;36mtraining_transformer\u001b[0;34m(model, optimizer, criterion, train_loader, test_loader, n_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m#Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Initialize Transformer Model and Optimizer\n",
        "\n",
        "model_transformer = Transformer (num_encoder_layers=3,\n",
        "                     num_decoder_layers=3,\n",
        "                     feature_size=18,\n",
        "                     output_size=18,\n",
        "                     num_heads=3,\n",
        "                     device = device,\n",
        "                     batch_first=False)\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "print('Setup model Ok')\n",
        "\n",
        "optimizer = torch.optim.Adam(model_transformer.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print('Setup optimizer Ok')\n",
        "\n",
        "\n",
        "train_transformer = True\n",
        "\n",
        "if train_transformer is True:\n",
        "  trained_model_transformer, train_losses, test_losses = training_transformer(model_transformer, optimizer, criterion, loader_train, loader_test, n_epochs)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Muhz9Q2qjDhs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "0cc459e6-70c5-45cf-da1d-b9d976da5ebb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgrbFBLS293A",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "5d0bbbab-f3d6-4152-e413-63b96685df52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "if train_transformer is True:\n",
        "\n",
        "  # Show results of the loss function\n",
        "\n",
        "  fig = plt.figure()\n",
        "\n",
        "  ax = fig.add_subplot(111)\n",
        "  plt.ion()\n",
        "\n",
        "  fig.show()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  baseline = [np.mean(losses_train) for i in range(len(train_losses))]\n",
        "\n",
        "  ax.plot(baseline)\n",
        "  ax.plot([np.mean(i) for i in train_losses])\n",
        "  ax.plot([np.mean(i) for i in test_losses])\n",
        "  ax.set_title(\"Mean Squared Error RNN\")\n",
        "  fig.canvas.draw()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequece_pairs(dataset_norm):\n",
        "\n",
        "  # Create a dataset with pairs data / Target (in this case data is one sequence of 30 measures (18 features) and target are the next sequence of 30 \n",
        "  # measures (18 features)). When you plug in one measure, the model should out the next measure\n",
        "\n",
        "  pair_set = []\n",
        "\n",
        "  for i in tqdm(range(len(dataset_norm) - 60)):\n",
        "    data = np.array(dataset_norm.iloc[i:i+30, 1:])\n",
        "    target = np.array(dataset_norm.iloc[i+30:i+60, 1:])\n",
        "    \n",
        "    pair_set.append((data, target))\n",
        "\n",
        "  dataset_pairs = np.array(pair_set)\n",
        "\n",
        "  training_data_pairs, testing_data_pairs = train_test_split(dataset_pairs, test_size=0.1, random_state=25)\n",
        "\n",
        "  data = []\n",
        "  target = []\n",
        "  for i in training_data_pairs:\n",
        "    data.append(i[0])\n",
        "    target.append(i[1])\n",
        "\n",
        "  training_data = torch.from_numpy(np.array(data)).float().to(device)\n",
        "  training_target = torch.from_numpy(np.array(target)).float().to(device)\n",
        "\n",
        "  data = []\n",
        "  target = []\n",
        "  for i in testing_data_pairs:\n",
        "    data.append(i[0])\n",
        "    target.append(i[1])\n",
        "\n",
        "  test_data = torch.from_numpy(np.array(data)).float().to(device)\n",
        "  test_target = torch.from_numpy(np.array(target)).float().to(device)\n",
        "\n",
        "  return (training_data, training_target), (test_data, test_target)"
      ],
      "metadata": {
        "id": "cEOlIPX4EcA1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs, test_pairs = create_sequece_pairs(dataset_norm)"
      ],
      "metadata": {
        "id": "0NyICzR4EpUB",
        "outputId": "45b6e0d3-81fc-4cff-9a18-33aa1d03160c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63300/63300 [00:34<00:00, 1841.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_pairs))\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "losses_train = []\n",
        "\n",
        "for i in train_pairs:\n",
        "  output = i[0]\n",
        "  target = i[1]\n",
        "  loss = criterion(output, target)\n",
        "  losses_train.append(loss.item())\n",
        "\n",
        "losses_test = []\n",
        "\n",
        "for i in test_pairs:\n",
        "  output = i[0]\n",
        "  target = i[1]\n",
        "  loss = criterion(output, target)\n",
        "  losses_test.append(loss.item())\n",
        "\n",
        "print(\"Training set\")\n",
        "print(\"Mean Loss of baselinemodel: \", np.mean(losses_train))\n",
        "print(\"Standard deviation Loss of baselinemodel: \", np.std(losses_train))\n",
        "print('\\n')\n",
        "print(\"Test set\")\n",
        "print(\"Mean Loss of baselinemodel: \", np.mean(losses_test))\n",
        "print(\"Standard deviation Loss of baselinemodel: \", np.std(losses_test))\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "a5-laMb6Dngn",
        "outputId": "08ecd839-5c09-4afa-813c-c6cca19e8ddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n",
            "Training set\n",
            "Mean Loss of baselinemodel:  0.8073254525661469\n",
            "Standard deviation Loss of baselinemodel:  0.048481136560440063\n",
            "\n",
            "\n",
            "Test set\n",
            "Mean Loss of baselinemodel:  0.3840979039669037\n",
            "Standard deviation Loss of baselinemodel:  0.11812558770179749\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_transformer(model, optimizer, criterion, train_loader, test_loader, n_epochs):\n",
        "  epoch_loss_train = []\n",
        "  epoch_loss_test = []\n",
        "\n",
        "  for e in range(1, n_epochs + 1):\n",
        "\n",
        "    print(f'Epoch: {e} of {n_epochs}')\n",
        "\n",
        "    print('Training:')\n",
        "    model.train()\n",
        "\n",
        "    for i in tqdm(train_loader):\n",
        "\n",
        "      # Initialize optimizer gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      input = i[0]\n",
        "\n",
        "      target = i[1]\n",
        "\n",
        "      net_out = model.forward(input)\n",
        "\n",
        "      #Compute loss\n",
        "      loss = criterion(net_out, target)\n",
        "\n",
        "      #Backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      #Optimization\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    print('\\nTest with training set')\n",
        "    losses_train = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i in tqdm(train_loader):\n",
        "        input = i[0]\n",
        "        target = i[1]\n",
        "\n",
        "        net_out = model.forward(input)\n",
        "\n",
        "        #Compute loss\n",
        "        losses_train.append (float(criterion(net_out, target).item()))\n",
        "\n",
        "    \n",
        "    print('\\nCurrent Mean loss Train Set: ', np.mean(losses_train))\n",
        "    epoch_loss_train.append(losses_train)\n",
        "\n",
        "    print('\\nTest with training set')\n",
        "    losses_test = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i in tqdm(test_loader):\n",
        "        input = i[0]\n",
        "        target = i[1]\n",
        "\n",
        "        net_out = model.forward(input)\n",
        "\n",
        "        #Compute loss\n",
        "        losses_test.append (float(criterion(net_out, target).item()))\n",
        "\n",
        "    print('\\nCurrent Mean loss Test Set: ', np.mean(losses_test))\n",
        "    epoch_loss_test.append(losses_test)\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "  return model, epoch_loss_train, epoch_loss_test"
      ],
      "metadata": {
        "id": "ZrzzO5EECjJI"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Transformer Model and Optimizer\n",
        "\n",
        "model_transformer = Transformer (num_encoder_layers=3,\n",
        "                     num_decoder_layers=3,\n",
        "                     feature_size=18,\n",
        "                     output_size=18,\n",
        "                     num_heads=3,\n",
        "                     device = device,\n",
        "                     batch_first=False)\n",
        "\n",
        "\n",
        "print('Setup model Ok')\n",
        "\n",
        "n_epochs = 50\n",
        "optimizer = torch.optim.Adam(model_transformer.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print('Setup optimizer Ok')\n",
        "\n",
        "print(train_pairs[0].shape)\n",
        "\n",
        "train_transformer = True\n",
        "\n",
        "if train_transformer is True:\n",
        "  trained_model_transformer, train_losses, test_losses = training_transformer(model_transformer, optimizer, criterion, train_pairs, test_pairs, n_epochs)"
      ],
      "metadata": {
        "id": "m2Vy5m7GCqvM",
        "outputId": "870c0863-ebe4-4715-c3e2-8639f03dee87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup model Ok\n",
            "Setup optimizer Ok\n",
            "torch.Size([56970, 30, 18])\n",
            "Epoch: 1 of 50\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-d0e20b412fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_transformer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mtrained_model_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-70-efe2077d6e12>\u001b[0m in \u001b[0;36mtraining_transformer\u001b[0;34m(model, optimizer, criterion, train_loader, test_loader, n_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;31m#Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-041dc401cab4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#src_pos = src + positional_encoding(src.shape[1], src.shape[2], self.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    348\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                            need_weights=False)[0]\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4966\u001b[0m     \u001b[0;31m# set up shape vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4967\u001b[0;31m     \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4968\u001b[0m     \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4969\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS8DcLC-293B"
      },
      "source": [
        "Ideas, things to remember, to search, etc...\n",
        "\n",
        "reconstruction, vergelich mit base line model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "bsc_arbeit.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python392jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}